{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Movement Recognition ‚Äî Engineer Lab (v2.5)\n",
    "\n",
    "### üß≠ Welcome\n",
    "In this lab you‚Äôll build a simple, honest movement classifier from **already cleaned and split data**.  \n",
    "We focus on understanding *how data turns into features* and *how models are evaluated fairly* ‚Äî without ML heavy lifting.\n",
    "\n",
    "This edition uses **`./data/cleaned/acc_mag_gyro/`** only. It contains **train/** and **test/** splits of movement data with **accelerometer, gyroscope, and magnetometer** readings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Before you begin ‚Äî running this notebook\n",
    "\n",
    "This notebook is designed to work **out-of-the-box** for engineers who want to explore machine learning hands-on.  \n",
    "You can run it locally in **Visual Studio Code** (recommended) or online in **Google Colab**.\n",
    "\n",
    "---\n",
    "\n",
    "### üß© Option 1 ‚Äî Run locally in **Visual Studio Code**\n",
    "\n",
    "#### **Step 1 ‚Äî Install prerequisites**\n",
    "You‚Äôll need:\n",
    "- Visual Studio Code\n",
    "- The **Python** and **Jupyter** extensions for VS Code\n",
    "- **Python ‚â• 3.10**\n",
    "\n",
    "üí° **Linux note:** you may need to install the virtual-environment module first:\n",
    "```bash\n",
    "sudo apt update\n",
    "sudo apt install python3-venv\n",
    "```\n",
    "\n",
    "Verify your installation:\n",
    "```bash\n",
    "python3 --version\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 2 ‚Äî Prepare your project folder**\n",
    "However you obtain this code, preferably cloning the repo, make sure you have the following:\n",
    "```\n",
    "Exploring_ML_Movement_v2_5.ipynb\n",
    "requirements.txt\n",
    "data/\n",
    "‚îî‚îÄ‚îÄ cleaned/\n",
    "    ‚îî‚îÄ‚îÄ acc_mag_gyro/\n",
    "        ‚îú‚îÄ‚îÄ train/\n",
    "        ‚îî‚îÄ‚îÄ test/\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 3 ‚Äî Create and activate a virtual environment**\n",
    "From the terminal inside your project folder:\n",
    "\n",
    "```bash\n",
    "python3 -m venv .venv\n",
    "```\n",
    "\n",
    "Activate it:\n",
    "\n",
    "- **macOS / Linux**\n",
    "  ```bash\n",
    "  source .venv/bin/activate\n",
    "  ```\n",
    "- **Windows (PowerShell)**\n",
    "  ```powershell\n",
    "  .venv\\Scripts\\Activate\n",
    "  ```\n",
    "\n",
    "If the `activate` script isn‚Äôt there, it usually means `python3-venv` wasn‚Äôt installed before creating the environment.  \n",
    "Install it, delete `.venv/`, and recreate it using the commands above.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 4 ‚Äî Install dependencies**\n",
    "```bash\n",
    "pip install --upgrade pip\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 5 ‚Äî Select the kernel in VS Code**\n",
    "When prompted to **Select Kernel**, choose the one that points to your new environment:\n",
    "```\n",
    "Python 3.x ('.venv': venv)\n",
    "```\n",
    "\n",
    "If nothing appears, run this in your activated terminal:\n",
    "```bash\n",
    "pip install ipykernel\n",
    "```\n",
    "Then restart VS Code and open the notebook again.  \n",
    "\n",
    "üí° You can confirm you‚Äôre using the right environment by running:\n",
    "```python\n",
    "!which python\n",
    "```\n",
    "It should print a path ending with `.venv/bin/python`.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 6 ‚Äî Run the notebook**\n",
    "You‚Äôre ready! Run cells one by one or choose **Run All**.  \n",
    "The first few cells will check your setup and print Python, NumPy, and Pandas versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, platform\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"OS:\", platform.platform())\n",
    "print(\"NumPy:\", np.__version__)\n",
    "print(\"Pandas:\", pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a7a77d",
   "metadata": {},
   "source": [
    "### üßÆ Option 2 ‚Äî Run in **Google Colab**\n",
    "If you prefer the cloud:\n",
    "1. Upload the notebook and the `data/cleaned/acc_mag_gyro/` folder to your Colab environment.  \n",
    "2. Run the setup cell that installs the dependencies:\n",
    "   ```bash\n",
    "   !pip install -r requirements.txt\n",
    "   ```\n",
    "3. Proceed through the notebook as usual."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356f4bdf",
   "metadata": {},
   "source": [
    "## 1) What data do we have? (movement-only, pre-split)\n",
    "\n",
    "We work with **movement** recordings (e.g., `walk`, `run`, `jump`, `pushup`). The recordings are made with a cell phone in a person's pocket. \n",
    "Data is already cleaned and split into **train/** and **test/**. Each file includes some combination of:\n",
    "- **Accelerometer** (`ax, ay, az`)\n",
    "- **Gyroscope** (`gx, gy, gz`)\n",
    "- **Magnetometer** (`mx, my, mz`)\n",
    "\n",
    "We‚Äôll derive the **activity label** from file names, keep consistent columns, and inspect class balance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "CONFIG = {\n",
    "    \"DATA_ROOT\": \"./data/cleaned/acc_mag_gyro\",  # fixed path per instruction\n",
    "    \"WINDOW_SAMPLES\": 200,   # ~2s @100 Hz (adjust if needed)\n",
    "    \"WINDOW_STRIDE\": 200,\n",
    "    \"RANDOM_SEED\": 42,\n",
    "    \"MODELS\": [\"knn\",\"logreg\",\"linearsvm\"],\n",
    "}\n",
    "\n",
    "def read_csv_any(fp: Path):\n",
    "    import pandas as pd\n",
    "    tries=[dict(),dict(sep=';'),dict(sep='\\t'),dict(engine='python'),\n",
    "           dict(engine='python',sep=';'),dict(engine='python',sep='\\t')]\n",
    "    last=None\n",
    "    for kw in tries:\n",
    "        try:\n",
    "            return pd.read_csv(fp, **kw)\n",
    "        except Exception as e:\n",
    "            last=e\n",
    "    raise last\n",
    "\n",
    "def normalize_schema(df: pd.DataFrame):\n",
    "    df = df.copy()\n",
    "    if \"Unnamed: 0\" in df.columns:\n",
    "        df = df.drop(columns=[\"Unnamed: 0\"])\n",
    "    # If generic XYZ present, map to accelerometer by default (these cleaned files should already be named, but be safe)\n",
    "    if set([\"X\",\"Y\",\"Z\"]).issubset(df.columns):\n",
    "        df = df.rename(columns={\"X\":\"ax\",\"Y\":\"ay\",\"Z\":\"az\"})\n",
    "    return df\n",
    "\n",
    "def derive_label_from_name(p: Path):\n",
    "    s = p.name.lower()\n",
    "    for token in [\"sit_to_stand\",\"stand_to_sit\",\"pushup\",\"jump\",\"walk\",\"run\",\"sit\",\"stand\",\"lie\"]:\n",
    "        if token in s: return token\n",
    "    # fallback: parent dir might be the label\n",
    "    parent = p.parent.name.lower()\n",
    "    for token in [\"pushup\",\"jump\",\"walk\",\"run\",\"sit\",\"stand\",\"lie\"]:\n",
    "        if token in parent: return token\n",
    "    return \"unknown\"\n",
    "\n",
    "def load_split(split: str):\n",
    "    root = Path(CONFIG[\"DATA_ROOT\"])/split\n",
    "    files = list(root.rglob(\"*.csv\"))\n",
    "    tables=[]\n",
    "    for f in files:\n",
    "        try:\n",
    "            df = read_csv_any(f)\n",
    "        except Exception as e:\n",
    "            print(\"[WARN] read failed:\", f, e); continue\n",
    "        df = normalize_schema(df)\n",
    "        df[\"__source_path\"] = str(f.relative_to(Path(CONFIG[\"DATA_ROOT\"])))\n",
    "        df[\"label\"] = derive_label_from_name(f)\n",
    "        tables.append(df)\n",
    "    assert tables, f\"No CSV files found under {root}\"\n",
    "    data = pd.concat(tables, ignore_index=True)\n",
    "    return data\n",
    "\n",
    "train_df = load_split(\"train\")\n",
    "test_df  = load_split(\"test\")\n",
    "\n",
    "print(\"Train rows:\", len(train_df), \"Test rows:\", len(test_df))\n",
    "print(\"Train cols:\", list(train_df.columns)[:12])\n",
    "print(\"Labels (train):\", sorted(train_df['label'].unique()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Explore labels and signals (visuals)\n",
    "\n",
    "First, let‚Äôs check **class balance** in the train split, then **peek at a few signals** from a random file to build intuition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "\n",
    "# Class counts\n",
    "train_counts = train_df['label'].value_counts().sort_index()\n",
    "display(train_counts)\n",
    "\n",
    "# Bar plot\n",
    "plt.figure(figsize=(6,3))\n",
    "train_counts.plot(kind='bar')\n",
    "plt.title(\"Train class counts\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Rows\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Quick look at available signal columns\n",
    "meta_cols = {\"__source_path\",\"label\",\"Timestamp\",\"Milliseconds\"}\n",
    "num_cols = [c for c in train_df.columns if c not in meta_cols and pd.api.types.is_numeric_dtype(train_df[c])]\n",
    "print(\"Numeric columns (sample):\", num_cols[:12])\n",
    "\n",
    "# Plot a sample recording's time series for intuition\n",
    "import random\n",
    "sample_path = random.choice(train_df['__source_path'].unique().tolist())\n",
    "sample_df = train_df[train_df['__source_path']==sample_path].reset_index(drop=True)\n",
    "print(\"Sample file:\", sample_path, \"Label:\", sample_df['label'].iloc[0])\n",
    "\n",
    "plt.figure(figsize=(8,3))\n",
    "if set([\"ax\",\"ay\",\"az\"]).issubset(sample_df.columns):\n",
    "    sample_df[[\"ax\",\"ay\",\"az\"]].plot(ax=plt.gca())\n",
    "    plt.title(\"Accelerometer (ax, ay, az) ‚Äî sample recording\")\n",
    "    plt.xlabel(\"Sample index\"); plt.ylabel(\"Acceleration\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "if set([\"gx\",\"gy\",\"gz\"]).issubset(sample_df.columns):\n",
    "    plt.figure(figsize=(8,3))\n",
    "    sample_df[[\"gx\",\"gy\",\"gz\"]].plot(ax=plt.gca())\n",
    "    plt.title(\"Gyroscope (gx, gy, gz) ‚Äî sample recording\")\n",
    "    plt.xlabel(\"Sample index\"); plt.ylabel(\"Angular velocity\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "if set([\"mx\",\"my\",\"mz\"]).issubset(sample_df.columns):\n",
    "    plt.figure(figsize=(8,3))\n",
    "    sample_df[[\"mx\",\"my\",\"mz\"]].plot(ax=plt.gca())\n",
    "    plt.title(\"Magnetometer (mx, my, mz) ‚Äî sample recording\")\n",
    "    plt.xlabel(\"Sample index\"); plt.ylabel(\"Magnetic field\")\n",
    "    plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Feature extraction ‚Äî windows ‚Üí summaries\n",
    "\n",
    "We convert each recording into **fixed-length windows** (e.g., 2 seconds) and summarize each window with simple statistics.  \n",
    "We‚Äôll use **accelerometer + gyroscope** by default (magnetometer is optional) for better movement cues.\n",
    "\n",
    "**Why windows?** Models work on fixed-size inputs; short windows capture short-lived patterns while keeping computation simple.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "\n",
    "WINDOW_SAMPLES = CONFIG[\"WINDOW_SAMPLES\"]\n",
    "WINDOW_STRIDE  = CONFIG[\"WINDOW_STRIDE\"]\n",
    "\n",
    "# Choose a consistent set of signals present in cleaned data\n",
    "base_signals = [c for c in [\"ax\",\"ay\",\"az\",\"gx\",\"gy\",\"gz\"] if c in train_df.columns]\n",
    "opt_signals  = [c for c in [\"mx\",\"my\",\"mz\"] if c in train_df.columns]  # optional\n",
    "signal_cols  = base_signals  # change to base_signals+opt_signals to include mag\n",
    "print(\"Using signals:\", signal_cols)\n",
    "\n",
    "def make_windows_from_table(df_in: pd.DataFrame, signal_cols, label_col, group_col, win, stride):\n",
    "    feats, labels, groups, sources = [], [], [], []\n",
    "    lab_cat = pd.Categorical(df_in[label_col])\n",
    "    for src, sdf in df_in.groupby(group_col, sort=False):\n",
    "        sdf = sdf.reset_index(drop=True)\n",
    "        if len(sdf) < win: \n",
    "            continue\n",
    "        X = sdf[signal_cols].to_numpy(dtype=float)\n",
    "        y_codes = pd.Categorical(sdf[label_col], categories=lab_cat.categories).codes\n",
    "        for start in range(0, len(sdf)-win+1, stride):\n",
    "            stop = start+win\n",
    "            seg = X[start:stop]\n",
    "            lab = pd.Series(y_codes[start:stop]).mode().iloc[0]\n",
    "            mu  = np.nanmean(seg, axis=0)\n",
    "            sd  = np.nanstd(seg, axis=0, ddof=1)\n",
    "            ptp = np.nanmax(seg, axis=0) - np.nanmin(seg, axis=0)\n",
    "            row = {}\n",
    "            for c,v in zip(signal_cols, mu):  row[f\"{c}_mean\"]=v\n",
    "            for c,v in zip(signal_cols, sd):  row[f\"{c}_std\"] =v\n",
    "            for c,v in zip(signal_cols, ptp): row[f\"{c}_ptp\"]=v\n",
    "            feats.append(row); labels.append(lab); groups.append(src); sources.append(src)\n",
    "    Xf = pd.DataFrame(feats)\n",
    "    y = np.asarray(labels)\n",
    "    meta = pd.DataFrame({\"__source_path\": sources})\n",
    "    return Xf, y, meta, list(lab_cat.categories)\n",
    "\n",
    "# Build windows from pre-split train/test independently\n",
    "train_Xf, train_y, train_meta, label_names = make_windows_from_table(\n",
    "    train_df, signal_cols, \"label\", \"__source_path\", WINDOW_SAMPLES, WINDOW_STRIDE\n",
    ")\n",
    "test_Xf,  test_y,  test_meta, _           = make_windows_from_table(\n",
    "    test_df,  signal_cols, \"label\", \"__source_path\", WINDOW_SAMPLES, WINDOW_STRIDE\n",
    ")\n",
    "\n",
    "print(\"Train features:\", train_Xf.shape, \"Test features:\", test_Xf.shape, \"Classes:\", label_names)\n",
    "display(train_Xf.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Feature insights ‚Äî correlations and a quick 2D view\n",
    "\n",
    "Let‚Äôs **see the relationships** between features and a **simple 2D projection** to build intuition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, matplotlib.pyplot as plt\n",
    "\n",
    "# Correlation heatmap (train only, to avoid leakage)\n",
    "corr = train_Xf.corr(numeric_only=True)\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.imshow(corr, aspect='auto', interpolation='nearest')\n",
    "plt.title(\"Feature correlation (train)\")\n",
    "plt.xlabel(\"Features\"); plt.ylabel(\"Features\")\n",
    "plt.colorbar()\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Optional: quick 2D projection using PCA\n",
    "from sklearn.decomposition import PCA\n",
    "X_proj = PCA(n_components=2, random_state=CONFIG[\"RANDOM_SEED\"]).fit_transform(train_Xf.values)\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.scatter(X_proj[:,0], X_proj[:,1], s=6)\n",
    "plt.title(\"PCA projection (train features)\")\n",
    "plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Train on train ‚Üí Evaluate on test (no leakage)\n",
    "\n",
    "We respect the provided split: **fit on train windows**, **score on test windows**.  \n",
    "We‚Äôll compare a few baseline models and visualize a **confusion matrix** to see which classes confuse each other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.base import clone\n",
    "\n",
    "def build_model(name: str):\n",
    "    if name==\"knn\":         clf=KNeighborsClassifier(n_neighbors=5)\n",
    "    elif name==\"logreg\":    clf=LogisticRegression(max_iter=2000)\n",
    "    elif name==\"linearsvm\": clf=LinearSVC()\n",
    "    else: raise ValueError(name)\n",
    "    return Pipeline([(\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "                     (\"scaler\", StandardScaler()),\n",
    "                     (\"clf\", clf)])\n",
    "\n",
    "Xtr = train_Xf.to_numpy(dtype=float)\n",
    "Xte = test_Xf.to_numpy(dtype=float)\n",
    "\n",
    "rows=[]\n",
    "best=None; best_metric=-1\n",
    "for name in CONFIG[\"MODELS\"]:\n",
    "    model = build_model(name)\n",
    "    model.fit(Xtr, train_y)\n",
    "    yhat = model.predict(Xte)\n",
    "    acc = accuracy_score(test_y, yhat)\n",
    "    f1m = f1_score(test_y, yhat, average=\"macro\")\n",
    "    rows.append({\"model\":name, \"acc\":acc, \"f1_macro\":f1m})\n",
    "    if f1m>best_metric:\n",
    "        best_metric=f1m; best=(name, model, yhat)\n",
    "\n",
    "leaderboard = pd.DataFrame(rows).sort_values(\"f1_macro\", ascending=False).reset_index(drop=True)\n",
    "display(leaderboard)\n",
    "\n",
    "best_name, best_model, best_yhat = best\n",
    "print(\"Best model:\", best_name)\n",
    "print(classification_report(test_y, best_yhat, target_names=[str(c) for c in label_names]))\n",
    "\n",
    "cm = confusion_matrix(test_y, best_yhat, labels=list(range(len(label_names))))\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.imshow(cm, interpolation=\"nearest\")\n",
    "plt.title(\"Confusion matrix (test)\")\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "plt.colorbar(); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Error analysis ‚Äî where does the model struggle?\n",
    "\n",
    "Per-class precision/recall highlights which movements are hardest. Use this to guide feature ideas (e.g., add angular features if `run` vs `walk` is confused).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "prec, rec, f1c, supp = precision_recall_fscore_support(test_y, best_yhat, labels=list(range(len(label_names))), zero_division=0)\n",
    "df_metrics = pd.DataFrame({\"label_id\":range(len(label_names)), \"label\":[label_names[i] for i in range(len(label_names))],\n",
    "                           \"precision\":prec, \"recall\":rec, \"f1\":f1c, \"support\":supp})\n",
    "display(df_metrics)\n",
    "\n",
    "plt.figure(figsize=(7,3))\n",
    "plt.bar(df_metrics[\"label\"], df_metrics[\"f1\"])\n",
    "plt.title(\"Per-class F1 (test)\")\n",
    "plt.xlabel(\"Class\"); plt.ylabel(\"F1\")\n",
    "plt.xticks(rotation=20); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Stretch: add simple features (magnitudes)\n",
    "\n",
    "Try adding vector **magnitudes** within the window (e.g., `‚àö(ax¬≤+ay¬≤+az¬≤)` mean/std/ptp).  \n",
    "Does this improve separation between `run` and `walk`? Between `jump` and the rest?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "\n",
    "def add_magnitudes(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_in.copy()\n",
    "    if set([\"ax\",\"ay\",\"az\"]).issubset(df.columns):\n",
    "        df[\"a_mag\"] = np.sqrt(df[\"ax\"]**2 + df[\"ay\"]**2 + df[\"az\"]**2)\n",
    "    if set([\"gx\",\"gy\",\"gz\"]).issubset(df.columns):\n",
    "        df[\"g_mag\"] = np.sqrt(df[\"gx\"]**2 + df[\"gy\"]**2 + df[\"gz\"]**2)\n",
    "    if set([\"mx\",\"my\",\"mz\"]).issubset(df.columns):\n",
    "        df[\"m_mag\"] = np.sqrt(df[\"mx\"]**2 + df[\"my\"]**2 + df[\"mz\"]**2)\n",
    "    return df\n",
    "\n",
    "train_df_aug = add_magnitudes(train_df)\n",
    "test_df_aug  = add_magnitudes(test_df)\n",
    "\n",
    "# Re-window with magnitudes included\n",
    "sig_aug = [c for c in [\"ax\",\"ay\",\"az\",\"gx\",\"gy\",\"gz\",\"a_mag\",\"g_mag\"] if c in train_df_aug.columns]\n",
    "print(\"Signals with magnitudes:\", sig_aug)\n",
    "\n",
    "def make_windows_table(df_in, signals, label_col, group_col, win, stride):\n",
    "    feats, labels, sources = [], [], []\n",
    "    lab_cat = pd.Categorical(df_in[label_col])\n",
    "    for src, sdf in df_in.groupby(group_col, sort=False):\n",
    "        sdf = sdf.reset_index(drop=True)\n",
    "        if len(sdf) < win: continue\n",
    "        X = sdf[signals].to_numpy(dtype=float)\n",
    "        y_codes = pd.Categorical(sdf[label_col], categories=lab_cat.categories).codes\n",
    "        for start in range(0, len(sdf)-win+1, stride):\n",
    "            stop = start+win\n",
    "            seg = X[start:stop]\n",
    "            lab = pd.Series(y_codes[start:stop]).mode().iloc[0]\n",
    "            mu=np.nanmean(seg,0); sd=np.nanstd(seg,0,ddof=1); ptp=np.nanmax(seg,0)-np.nanmin(seg,0)\n",
    "            row={}\n",
    "            for c,v in zip(signals, mu):  row[f\"{c}_mean\"]=v\n",
    "            for c,v in zip(signals, sd):  row[f\"{c}_std\"]=v\n",
    "            for c,v in zip(signals, ptp): row[f\"{c}_ptp\"]=v\n",
    "            feats.append(row); labels.append(lab); sources.append(src)\n",
    "    return pd.DataFrame(feats), np.asarray(labels)\n",
    "\n",
    "train_Xf2, train_y2 = make_windows_table(train_df_aug, sig_aug, \"label\", \"__source_path\", CONFIG[\"WINDOW_SAMPLES\"], CONFIG[\"WINDOW_STRIDE\"])\n",
    "test_Xf2,  test_y2  = make_windows_table(test_df_aug,  sig_aug, \"label\", \"__source_path\", CONFIG[\"WINDOW_SAMPLES\"], CONFIG[\"WINDOW_STRIDE\"])\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "pipe = Pipeline([(\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "                 (\"scale\", StandardScaler()),\n",
    "                 (\"clf\", LogisticRegression(max_iter=2000))])\n",
    "\n",
    "pipe.fit(train_Xf2, train_y2)\n",
    "yhat2 = pipe.predict(test_Xf2)\n",
    "print(\"Macro-F1 with magnitudes:\", f1_score(test_y2, yhat2, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Wrap-up ‚Äî what you built\n",
    "\n",
    "- Turned **clean, split sensor data** into windowed features.\n",
    "- Compared a few **baseline models** and read a **confusion matrix**.\n",
    "- Used visuals to understand **class balance**, **signal shapes**, **feature relationships**.\n",
    "- Tried a small feature idea (magnitudes) and measured its impact.\n",
    "\n",
    "> The same engineering mindset ‚Äî clean inputs, fair evaluation, modest baselines, careful iteration ‚Äî scales all the way up to modern AI systems.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
