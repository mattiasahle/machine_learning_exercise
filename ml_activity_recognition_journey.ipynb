{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ Machine Learning Journey: From Sensor Data to Intelligence\n",
    "\n",
    "## Welcome to Your ML Adventure!\n",
    "\n",
    "**What you're about to build:** A neural network that recognizes human activities (running, walking, jumping, push-ups) from smartphone sensor data - the same technology powering fitness trackers, smartwatches, and health apps worldwide.\n",
    "\n",
    "**Why this matters in 2025:** You're living through the AI revolution. ChatGPT, GitHub Copilot, and countless AI tools are transforming how we work, learn, and create. But what makes them tick? **The same fundamental principles you'll master in this lab.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Your Learning Path\n",
    "\n",
    "This notebook is structured as an **exploratory journey**, not just a code dump. You'll:\n",
    "\n",
    "1. **ðŸ”§ Prepare** - Load and clean real sensor data\n",
    "2. **ðŸ” Explore** - Visualize patterns and understand what makes activities different\n",
    "3. **ðŸ—ï¸ Build** - Construct an LSTM neural network from scratch\n",
    "4. **ðŸ“Š Evaluate** - Measure performance with industry-standard metrics\n",
    "5. **ðŸ§ª Experiment** - Test different architectures and hyperparameters\n",
    "6. **ðŸŽ“ Connect** - Understand how these techniques power modern AI tools\n",
    "\n",
    "### ðŸ’¡ How to Use This Notebook\n",
    "\n",
    "**ðŸ“– Read the markdown cells carefully** - They explain concepts, ask questions, and guide your thinking\n",
    "\n",
    "**â–¶ï¸ Run each code cell in order** - Machine learning is sequential; each step builds on the previous\n",
    "\n",
    "**ðŸ¤” Stop and think** - When you see questions or insights, pause and reflect\n",
    "\n",
    "**ðŸ”¬ Experiment** - After completing the lab, try changing parameters and see what happens\n",
    "\n",
    "**â“ Ask questions** - Whether to yourself, peers, or AI tools - curiosity drives learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŒŸ The Big Picture\n",
    "\n",
    "### What Makes This Exciting?\n",
    "\n",
    "You're not just learning to code - you're learning to **teach machines to learn**. The LSTM architecture you'll build today is a direct ancestor of the Transformers powering GPT-4, Claude, and other breakthrough AI systems.\n",
    "\n",
    "**Think about it:**\n",
    "- Your model will process **256 timesteps** of sensor data\n",
    "- GPT-4 processes **128,000+ tokens** of text\n",
    "- **Same core concepts**: sequential processing, pattern recognition, prediction\n",
    "\n",
    "The difference? Scale, not principle.\n",
    "\n",
    "### The Journey Ahead\n",
    "\n",
    "By the end of this notebook, you'll have:\n",
    "\n",
    "âœ… Built a working AI system from raw data to predictions  \n",
    "âœ… Understood evaluation metrics used by ML engineers worldwide  \n",
    "âœ… Experimented with the same hyperparameters that optimize billion-parameter models  \n",
    "âœ… Developed intuition for **when** and **why** AI systems work (or fail)\n",
    "\n",
    "### ðŸŽ A Promise\n",
    "\n",
    "**Stick with this journey to the end**, and you'll discover something profound: The techniques you're learning here are **exactly** what's powering the AI revolution happening around you right now. \n",
    "\n",
    "The conclusion of this notebook will connect the dots between your 200,000-parameter LSTM and the multi-trillion-parameter models changing the world - and explain why understanding these fundamentals makes you more than just an AI user.\n",
    "\n",
    "**Ready? Let's begin! ðŸš€**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“‘ Table of Contents\n",
    "\n",
    "### Part 1: Foundation\n",
    "1. **Setup** - Import libraries and configure environment\n",
    "2. **Functions** - Helper functions for data loading and math\n",
    "3. **Data Preparation** - Load and structure sensor data\n",
    "\n",
    "### Part 2: Data Exploration\n",
    "4. **Exploratory Data Analysis** - Visualize class distributions and sensor patterns\n",
    "5. **Understanding the Data** - Statistical analysis and feature exploration\n",
    "\n",
    "### Part 3: Model Development\n",
    "6. **RNN Classification** - Build LSTM neural network\n",
    "7. **Model Architecture** - Design and compile the model\n",
    "8. **Training** - Fit the model on training data\n",
    "9. **Evaluation** - Test performance and analyze results\n",
    "\n",
    "### Part 4: Deep Dive\n",
    "10. **Understanding the Confusion Matrix** - Interpret classification results\n",
    "11. **Detailed Performance Metrics** - Precision, recall, F1-scores per class\n",
    "12. **Analyzing Misclassifications** - Understand where the model struggles\n",
    "\n",
    "### Part 5: Real-World Testing\n",
    "13. **Sequential Prediction** - Test on continuous activity recordings\n",
    "14. **Prediction Confidence Analysis** - Examine model certainty\n",
    "\n",
    "### Part 6: Experiments\n",
    "15. **Experiment 1: LSTM vs GRU** - Compare recurrent architectures\n",
    "16. **Experiment 2: Timestep Window Size** - Test different context lengths\n",
    "17. **Experiment 3: Dropout Effect** - Analyze regularization impact\n",
    "\n",
    "### Part 7: The Big Reveal\n",
    "18. **Conclusion: From Fundamentals to the AI Revolution** - Connect your LSTM to GPT-4 and modern AI\n",
    "\n",
    "**Estimated completion time:** 2-3 hours\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Configure TensorFlow to use CPU only (avoid CUDA issues)\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(filepath, cols, names):\n",
    "    df = pd.read_csv(filepath, usecols = cols, names = names, header = None)\n",
    "    df = df.drop([0], axis = 0) # Remove first row (headers)\n",
    "    df = df.astype('float64') # Cast data to floats\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class(file_path):\n",
    "    return '_'.join(file_path.split('/')[-1].split('_')[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_df(file_path, df):\n",
    "    class_name = get_class(file_path)\n",
    "\n",
    "    return pd.DataFrame({'class': [class_name for _ in range(len(df))]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_acc_mag_csvs():\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    \n",
    "    for file in glob.glob('data/cleaned/acc_mag/*.csv'):\n",
    "\n",
    "        if 'acc' in file.split('/')[-1]:\n",
    "            mag_file = file.replace(file.split('/')[-1], file.split('/')[-1].replace('acc', 'mag'))\n",
    "            \n",
    "            df_acc = read_csv(file, [1,2,3], ['ax', 'ay', 'az'])\n",
    "            df_mag = read_csv(mag_file, [1,2,3], ['mx', 'my', 'mz'])\n",
    "            \n",
    "            df = pd.concat([df_acc, df_mag], axis=1, join='inner')\n",
    "            X_list.append(df)\n",
    "            y_list.append(get_class_df(file, df))\n",
    "\n",
    "    X = pd.concat(X_list, ignore_index=True) if X_list else pd.DataFrame()\n",
    "    y = pd.concat(y_list, ignore_index=True) if y_list else pd.DataFrame(columns=['class'])\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_acc_mag_gyro_csvs(set, acc = True, mag = True, gyro = True, filter=''):\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    \n",
    "    for file in glob.glob('data/cleaned/acc_mag_gyro/' + set + '/*' + filter + '*.csv'):\n",
    "\n",
    "        if acc or mag or gyro:\n",
    "            if 'acc' in file.split('/')[-1]:\n",
    "                dfs = []\n",
    "                \n",
    "                if acc:\n",
    "                    df_acc = read_csv(file, [1,2,3], ['ax', 'ay', 'az'])\n",
    "                    if mag: df_acc = df_acc.groupby(np.arange(len(df_acc))//10).mean()\n",
    "                    dfs.append(df_acc)\n",
    "                if mag:\n",
    "                    mag_file = file.replace(file.split('/')[-1], file.split('/')[-1].replace('acc', 'mag'))\n",
    "                    df_mag = read_csv(mag_file, [1,2,3], ['mx', 'my', 'mz'])\n",
    "                    dfs.append(df_mag)\n",
    "                if gyro:\n",
    "                    gyro_file = file.replace(file.split('/')[-1], file.split('/')[-1].replace('acc', 'gyro'))\n",
    "                    df_gyro = read_csv(gyro_file, [1,2,3], ['gx', 'gy', 'gz'])\n",
    "                    if mag: df_gyro = df_gyro.groupby(np.arange(len(df_gyro))//10).mean()\n",
    "                    dfs.append(df_gyro)\n",
    "                \n",
    "                df = pd.concat(dfs, axis=1, join='inner')\n",
    "                X_list.append(df)\n",
    "                y_list.append(get_class_df(file, df))\n",
    "    \n",
    "    X = pd.concat(X_list, ignore_index=True) if X_list else pd.DataFrame()\n",
    "    y = pd.concat(y_list, ignore_index=True) if y_list else pd.DataFrame(columns=['class'])\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filepaths_from_folder(path, filter=''):\n",
    "    files = []\n",
    "    \n",
    "    for file in glob.glob(path + '*'):\n",
    "        if filter in file.split('/')[-1]:\n",
    "            files.append(file)\n",
    "        \n",
    "    return files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_norms(df):\n",
    "    l = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        l.append(np.linalg.norm(df.iloc[i]))\n",
    "        \n",
    "    return pd.DataFrame({'Norm': l})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_angle(df, axis, i):\n",
    "    return np.rad2deg(np.arccos(df[axis].iloc[i] / np.linalg.norm(df.iloc[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_angles(df, axis):\n",
    "    l = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        l.append(calc_angle(df, axis, i))\n",
    "        \n",
    "    return pd.DataFrame({'Angle': l})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                        normalize=False,\n",
    "                        title='Confusion matrix',\n",
    "                        cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for plotting\n",
    "# Concatenate test data with its predictions and rename classes from categorical to numerical.\n",
    "\n",
    "def prep_data_for_all_move_plot(df_test, df_pred):\n",
    "    df = pd.concat([df_test, df_pred], axis=1)\n",
    "    df = df.rename(columns={0: 'pred'})\n",
    "    # Use map instead of replace to avoid downcasting warning\n",
    "    mapping = {'run': 0, 'walk': 1, 'jump': 2, 'pushup': 3}\n",
    "    df = df.assign(pred=df['pred'].map(mapping))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_move_with_pred(df_test, df_pred, col, cmap = matplotlib.colormaps.get_cmap(\"viridis\")):\n",
    "    df = prep_data_for_all_move_plot(df_test, df_pred)\n",
    "\n",
    "    plot = df[col].plot(\n",
    "        figsize=(25,5), \n",
    "        color='black', \n",
    "        title='', \n",
    "        legend=True, \n",
    "        xlim=(0,len(df)), \n",
    "        xlabel='Time', \n",
    "        xticks=[], \n",
    "        ylabel='Data')\n",
    "    fig = plot.pcolorfast(\n",
    "        plot.get_xlim(), \n",
    "        plot.get_ylim(), \n",
    "        df['pred'].values[np.newaxis], \n",
    "        cmap=cmap,\n",
    "        alpha=0.7)\n",
    "    colorbar = plt.colorbar(fig, ticks=[0.4,1.1,1.85,2.6], label='Prediction')\n",
    "    colorbar.ax.set_yticklabels(['run', 'walk', 'jump', 'pushup'])\n",
    "\n",
    "    print('              run                   walk              jump          pushup             run                     walk              jump        pushup')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_shapes_train():\n",
    "    print('Shapes')\n",
    "    print('X_train_run:\\t', X_train_run.shape, '\\t\\ty_train_run:\\t', y_train_run.shape)\n",
    "    print('X_train_walk:\\t', X_train_walk.shape, '\\t\\ty_train_walk:\\t', y_train_walk.shape)\n",
    "    print('X_train_jump:\\t', X_train_jump.shape, '\\t\\ty_train_jump:\\t', y_train_jump.shape)\n",
    "    print('X_train_pushup:\\t', X_train_pushup.shape, '\\t\\ty_train_pushup:\\t', y_train_pushup.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_shapes_test():\n",
    "    print('Shapes')\n",
    "    print('X_test_run:\\t', X_test_run.shape, '\\t\\ty_test_run:\\t', y_test_run.shape)\n",
    "    print('X_test_walk:\\t', X_test_walk.shape, '\\t\\ty_test_walk:\\t', y_test_walk.shape)\n",
    "    print('X_test_jump:\\t', X_test_jump.shape, '\\t\\ty_test_jump:\\t', y_test_jump.shape)\n",
    "    print('X_test_pushup:\\t', X_test_pushup.shape, '\\t\\ty_test_pushup:\\t', y_test_pushup.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and append"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw csv's read into dataframes divided by class\n",
    "\n",
    "X_train_run, y_train_run = read_acc_mag_gyro_csvs('train', acc=True, mag=False, gyro=True, filter='run')\n",
    "X_train_walk, y_train_walk = read_acc_mag_gyro_csvs('train', acc=True, mag=False, gyro=True, filter='walk')\n",
    "X_train_jump, y_train_jump = read_acc_mag_gyro_csvs('train', acc=True, mag=False, gyro=True, filter='jump')\n",
    "X_train_pushup, y_train_pushup = read_acc_mag_gyro_csvs('train', acc=True, mag=False, gyro=True, filter='pushup')\n",
    "\n",
    "print_shapes_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum number of rows\n",
    "# We balance all classes to the same size to prevent bias toward overrepresented activities\n",
    "\n",
    "n_rows_train = 7168\n",
    "n_rows_train/256     # Test that n_rows is evenly divisible by a number which is a factor of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows > n_rows\n",
    "\n",
    "X_train_run, y_train_run = X_train_run[:n_rows_train], y_train_run[:n_rows_train]\n",
    "X_train_walk, y_train_walk = X_train_walk[:n_rows_train], y_train_walk[:n_rows_train]\n",
    "X_train_jump, y_train_jump = X_train_jump[:n_rows_train], y_train_jump[:n_rows_train]\n",
    "X_train_pushup, y_train_pushup = X_train_pushup[:n_rows_train], y_train_pushup[:n_rows_train]\n",
    "\n",
    "print_shapes_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append all classes in one dataframe\n",
    "\n",
    "X_train_df = pd.concat([X_train_run, X_train_walk, X_train_jump, X_train_pushup], ignore_index=True)\n",
    "y_train_df = pd.concat([y_train_run, y_train_walk, y_train_jump, y_train_pushup], ignore_index=True)\n",
    "\n",
    "X_train_df.shape, y_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics for each activity\n",
    "activities_list = [('run', X_train_run), ('walk', X_train_walk), ('jump', X_train_jump), ('pushup', X_train_pushup)]\n",
    "\n",
    "print(\"ðŸ“Š Statistical Summary by Activity:\\n\")\n",
    "for activity_name, X_data in activities_list:\n",
    "    print(f\"\\n{activity_name.upper()}:\")\n",
    "    print(f\"  Accelerometer (ax, ay, az) - Mean: {X_data[['ax','ay','az']].mean().values}\")\n",
    "    print(f\"  Accelerometer (ax, ay, az) - Std:  {X_data[['ax','ay','az']].std().values}\")\n",
    "    print(f\"  Gyroscope (gx, gy, gz) - Mean: {X_data[['gx','gy','gz']].mean().values}\")\n",
    "    print(f\"  Gyroscope (gx, gy, gz) - Std:  {X_data[['gx','gy','gz']].std().values}\")\n",
    "\n",
    "# Visualize standard deviations\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "std_data = []\n",
    "for activity_name, X_data in activities_list:\n",
    "    std_data.append({\n",
    "        'Activity': activity_name,\n",
    "        'ax': X_data['ax'].std(),\n",
    "        'ay': X_data['ay'].std(),\n",
    "        'az': X_data['az'].std(),\n",
    "        'gx': X_data['gx'].std(),\n",
    "        'gy': X_data['gy'].std(),\n",
    "        'gz': X_data['gz'].std()\n",
    "    })\n",
    "\n",
    "std_df = pd.DataFrame(std_data).set_index('Activity')\n",
    "std_df.plot(kind='bar', ax=ax, color=['#e74c3c', '#3498db', '#2ecc71', '#9b59b6', '#e67e22', '#1abc9c'])\n",
    "ax.set_title('Standard Deviation by Activity and Sensor Axis', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Activity')\n",
    "ax.set_ylabel('Standard Deviation')\n",
    "ax.legend(title='Sensor Axis', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Insight: Standard deviation is a good feature!\")\n",
    "print(\"   Different activities show distinct variation patterns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Summary\n",
    "\n",
    "Quantifying differences between activities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sample sensor data for each activity\n",
    "fig, axes = plt.subplots(4, 2, figsize=(16, 12))\n",
    "activities = [('run', X_train_run), ('walk', X_train_walk), ('jump', X_train_jump), ('pushup', X_train_pushup)]\n",
    "colors_acc = ['#e74c3c', '#3498db', '#2ecc71']\n",
    "colors_gyro = ['#9b59b6', '#e67e22', '#1abc9c']\n",
    "\n",
    "sample_size = 500  # Show first 500 samples\n",
    "\n",
    "for idx, (activity_name, X_data) in enumerate(activities):\n",
    "    X_sample = X_data[:sample_size]\n",
    "    \n",
    "    # Plot accelerometer data\n",
    "    ax_acc = axes[idx, 0]\n",
    "    ax_acc.plot(X_sample['ax'], label='ax', color=colors_acc[0], alpha=0.7, linewidth=1)\n",
    "    ax_acc.plot(X_sample['ay'], label='ay', color=colors_acc[1], alpha=0.7, linewidth=1)\n",
    "    ax_acc.plot(X_sample['az'], label='az', color=colors_acc[2], alpha=0.7, linewidth=1)\n",
    "    ax_acc.set_title(f'{activity_name.upper()} - Accelerometer', fontsize=12, fontweight='bold')\n",
    "    ax_acc.set_ylabel('Acceleration (m/sÂ²)')\n",
    "    ax_acc.legend(loc='upper right')\n",
    "    ax_acc.grid(alpha=0.3)\n",
    "    \n",
    "    # Plot gyroscope data\n",
    "    ax_gyro = axes[idx, 1]\n",
    "    ax_gyro.plot(X_sample['gx'], label='gx', color=colors_gyro[0], alpha=0.7, linewidth=1)\n",
    "    ax_gyro.plot(X_sample['gy'], label='gy', color=colors_gyro[1], alpha=0.7, linewidth=1)\n",
    "    ax_gyro.plot(X_sample['gz'], label='gz', color=colors_gyro[2], alpha=0.7, linewidth=1)\n",
    "    ax_gyro.set_title(f'{activity_name.upper()} - Gyroscope', fontsize=12, fontweight='bold')\n",
    "    ax_gyro.set_ylabel('Angular Velocity (rad/s)')\n",
    "    ax_gyro.legend(loc='upper right')\n",
    "    ax_gyro.grid(alpha=0.3)\n",
    "    \n",
    "    if idx == 3:\n",
    "        ax_acc.set_xlabel('Time (samples)')\n",
    "        ax_gyro.set_xlabel('Time (samples)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ” Observations:\")\n",
    "print(\"- Each activity has distinct sensor patterns\")\n",
    "print(\"- Running shows high frequency oscillations\")\n",
    "print(\"- Walking has similar but lower amplitude patterns\")\n",
    "print(\"- Jumping shows periodic high-amplitude spikes\")\n",
    "print(\"- Push-ups have distinct phases (up/down)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Raw Sensor Data\n",
    "\n",
    "Let's see what the sensor patterns look like for each activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution in training data\n",
    "print(\"Training Set Class Distribution:\")\n",
    "print(y_train_df['class'].value_counts())\n",
    "print(f\"\\nTotal samples: {len(y_train_df)}\")\n",
    "print(f\"Samples per class: {len(y_train_df) // len(y_train_df['class'].unique())}\")\n",
    "\n",
    "# Visualize class distribution\n",
    "y_train_df['class'].value_counts().plot(kind='bar', color=['#2ecc71', '#3498db', '#e74c3c', '#f39c12'], figsize=(10, 5))\n",
    "plt.title('Training Set - Class Distribution', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Activity Class')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Our dataset is perfectly balanced - each class has equal representation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Distribution\n",
    "\n",
    "Let's verify our dataset is balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ðŸ“Š Data Exploration\n",
    "\n",
    "Before continuing with test data, let's explore our training data to understand what we're working with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw csv's read into dataframes divided by class\n",
    "\n",
    "X_test_run, y_test_run = read_acc_mag_gyro_csvs('test', acc=True, mag=False, gyro=True, filter='run')\n",
    "X_test_walk, y_test_walk = read_acc_mag_gyro_csvs('test', acc=True, mag=False, gyro=True, filter='walk')\n",
    "X_test_jump, y_test_jump = read_acc_mag_gyro_csvs('test', acc=True, mag=False, gyro=True, filter='jump')\n",
    "X_test_pushup, y_test_pushup = read_acc_mag_gyro_csvs('test', acc=True, mag=False, gyro=True, filter='pushup')\n",
    "\n",
    "print_shapes_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum number of rows\n",
    "\n",
    "n_rows_test = 1280\n",
    "n_rows_test/256     # Test that n_rows is evenly divisible by a number which is a factor of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows > n_rows\n",
    "\n",
    "X_test_run, y_test_run = X_test_run[:n_rows_test], y_test_run[:n_rows_test]\n",
    "X_test_walk, y_test_walk = X_test_walk[:n_rows_test], y_test_walk[:n_rows_test]\n",
    "X_test_jump, y_test_jump = X_test_jump[:n_rows_test], y_test_jump[:n_rows_test]\n",
    "X_test_pushup, y_test_pushup = X_test_pushup[:n_rows_test], y_test_pushup[:n_rows_test]\n",
    "\n",
    "print_shapes_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append all classes in one dataframe\n",
    "\n",
    "X_test_df = pd.concat([X_test_run, X_test_walk, X_test_jump, X_test_pushup], ignore_index=True)\n",
    "y_test_df = pd.concat([y_test_run, y_test_walk, y_test_jump, y_test_pushup], ignore_index=True)\n",
    "\n",
    "X_test_df.shape, y_test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test set - all movements combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_all_df, y_test_all_df = read_acc_mag_gyro_csvs('test/all', acc=True, mag=False, gyro=True)\n",
    "\n",
    "X_test_all_df.shape, y_test_all_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum number of rows\n",
    "\n",
    "n_rows_all = 20480\n",
    "n_rows_all/256     # Test that n_rows is evenly divisible by a number which is a factor of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows > n_rows\n",
    "\n",
    "X_test_all_df, y_test_all_df = X_test_all_df[:n_rows_all], y_test_all_df[:n_rows_all]\n",
    "\n",
    "X_test_all_df.shape, y_test_all_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get dataframe with stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import stdev\n",
    "\n",
    "def calc_stdev(data, n):\n",
    "    l = []\n",
    "    count = 0\n",
    "\n",
    "    while count <= len(data):\n",
    "        count += 1\n",
    "\n",
    "        if count % n == 0:\n",
    "            l.append(stdev(data[count-n:count]))\n",
    "\n",
    "    return pd.DataFrame({'stdev': l})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stdev_dfs(df_x, df_y, n):\n",
    "    df = pd.concat([df_x, df_y], axis=1)\n",
    "    classes = df['class'].unique()\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "\n",
    "    for i in range(0, len(classes)):\n",
    "        df_temp = calc_stdev(df[df['class'] == classes[i]]['ay'], n)\n",
    "        X_list.append(df_temp)\n",
    "        y_list.append(pd.DataFrame({'class': [classes[i] for _ in range(len(df_temp))]}))\n",
    "\n",
    "    X = pd.concat(X_list, ignore_index=True)\n",
    "    y = pd.concat(y_list, ignore_index=True)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_timesteps = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape df's, convert to numpy arrays and encode y\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Reshape and convert X\n",
    "X_train = np.reshape(X_train_df.to_numpy(), (X_train_df.shape[0] // n_timesteps, n_timesteps, X_train_df.shape[1]))\n",
    "X_test = np.reshape(X_test_df.to_numpy(), (X_test_df.shape[0] // n_timesteps, n_timesteps, X_test_df.shape[1]))\n",
    "X_test_all = np.reshape(X_test_all_df.to_numpy(), (X_test_all_df.shape[0] // n_timesteps, n_timesteps, X_test_all_df.shape[1]))\n",
    "X_test_all_stdev, y_test_all_stdev = get_stdev_dfs(X_test_all_df, y_test_all_df, n_timesteps)\n",
    "\n",
    "# One-hot encode y\n",
    "enc = LabelEncoder()\n",
    "\n",
    "y_train_np = y_train_df['class'][::n_timesteps].to_numpy()\n",
    "y_train_np_reshaped_labelenc = enc.fit_transform(y_train_np)\n",
    "y_train = keras.utils.to_categorical(y_train_np_reshaped_labelenc)\n",
    "\n",
    "y_test_np = y_test_df['class'][::n_timesteps].to_numpy()\n",
    "y_test_np_reshaped_labelenc = enc.fit_transform(y_test_np)\n",
    "y_test = keras.utils.to_categorical(y_test_np_reshaped_labelenc)\n",
    "\n",
    "print(f'\\t\\tX.shape\\t\\ty.shape')\n",
    "print(f'Train:\\t\\t{X_train.shape}\\t{y_train.shape}')\n",
    "print(f'Test:\\t\\t{X_test.shape}\\t{y_test.shape}')\n",
    "print(f'Test all:\\t{X_test_all.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Data Shape\n",
    "\n",
    "**Why reshape the data?**\n",
    "- LSTMs expect 3D input: (samples, timesteps, features)\n",
    "- We're converting continuous sensor readings into sequences of fixed length (256 timesteps)\n",
    "- Each sequence represents a \"window\" of activity\n",
    "\n",
    "**Why one-hot encode labels?**\n",
    "- Neural networks work better with categorical encoding\n",
    "- Converts ['run', 'walk', ...] â†’ [[1,0,0,0], [0,1,0,0], ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Understanding the Architecture\n",
    "\n",
    "**LSTM (Long Short-Term Memory):**\n",
    "- Designed for sequential data (perfect for time series!)\n",
    "- Can \"remember\" patterns over time\n",
    "- Better than simple RNNs at learning long-term dependencies\n",
    "\n",
    "**Why 256 units?**\n",
    "- Matches our timestep window size\n",
    "- Provides enough capacity to learn complex patterns\n",
    "- Not too large (would overfit) or too small (would underfit)\n",
    "\n",
    "**Dropout Layer (0.1):**\n",
    "- Randomly \"turns off\" 10% of neurons during training\n",
    "- Prevents overfitting by forcing the network to learn robust features\n",
    "- Acts as regularization\n",
    "\n",
    "**Softmax Output:**\n",
    "- Converts raw scores into probabilities (sum to 1.0)\n",
    "- Each output represents probability for each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ðŸ§  Model Architecture Exploration\n",
    "\n",
    "Let's explore different model architectures to understand what works best for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, GRU, Input\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer\n",
    "model.add(Input(shape=(X_train.shape[1], X_train.shape[2])))\n",
    "\n",
    "# Hidden layer\n",
    "model.add(LSTM(units=n_timesteps))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# Final layer\n",
    "model.add(Dense(units=4, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“ Model Summary Analysis\n",
    "\n",
    "Look at the model summary above:\n",
    "- **Total params**: ~330K parameters to train\n",
    "- **Most parameters** are in the LSTM layer (learns temporal patterns)\n",
    "- **Output layer** maps LSTM outputs to 4 activity classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10, shuffle=True, validation_split=0.1, verbose=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed training history analysis\n",
    "history_df = pd.DataFrame(history.history)\n",
    "\n",
    "print(\"ðŸŽ¯ Training Results Summary:\")\n",
    "print(f\"Final Training Accuracy: {history_df['accuracy'].iloc[-1]:.2%}\")\n",
    "print(f\"Final Validation Accuracy: {history_df['val_accuracy'].iloc[-1]:.2%}\")\n",
    "print(f\"Final Training Loss: {history_df['loss'].iloc[-1]:.4f}\")\n",
    "print(f\"Final Validation Loss: {history_df['val_loss'].iloc[-1]:.4f}\")\n",
    "\n",
    "# Check for overfitting\n",
    "accuracy_gap = history_df['accuracy'].iloc[-1] - history_df['val_accuracy'].iloc[-1]\n",
    "if accuracy_gap < 0.05:\n",
    "    print(\"\\nâœ… Good generalization - no significant overfitting!\")\n",
    "elif accuracy_gap < 0.10:\n",
    "    print(\"\\nâš ï¸  Slight overfitting detected (but acceptable)\")\n",
    "else:\n",
    "    print(\"\\nâŒ Significant overfitting - model memorizing training data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“ˆ Training Progress Analysis\n",
    "\n",
    "The plot above shows:\n",
    "- **Loss**: How wrong the model's predictions are (lower is better)\n",
    "- **Accuracy**: Percentage of correct predictions (higher is better)\n",
    "- **Training vs Validation**: \n",
    "  - If validation curve is much worse â†’ overfitting\n",
    "  - If both curves are bad â†’ underfitting\n",
    "  - If both curves are good and close â†’ just right! âœ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "ev = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Loss = {ev[0]:.2f}\\tAccuracy = {ev[1]:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ðŸ“Š Model Evaluation & Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_true=y_test_np, y_pred=enc.inverse_transform(model.predict(X_test).argmax(axis=1)))\n",
    "plot_confusion_matrix(cm=cm, classes=np.unique(y_test_np), title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Understanding the Confusion Matrix\n",
    "\n",
    "**How to read it:**\n",
    "- Rows = True labels (actual activity)\n",
    "- Columns = Predicted labels (what model said)\n",
    "- Diagonal = Correct predictions âœ…\n",
    "- Off-diagonal = Mistakes âŒ\n",
    "\n",
    "**What to look for:**\n",
    "- High diagonal values = Good accuracy\n",
    "- Patterns in mistakes reveal what the model confuses\n",
    "- Example: If model confuses \"walk\" with \"run\", they might have similar sensor patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze prediction confidence\n",
    "predictions = enc.inverse_transform(model.predict(X_test).argmax(axis=1))\n",
    "prediction_probs = model.predict(X_test)\n",
    "max_probs = prediction_probs.max(axis=1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Confidence distribution\n",
    "axes[0].hist(max_probs, bins=50, color='#3498db', alpha=0.7, edgecolor='black')\n",
    "axes[0].axvline(max_probs.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {max_probs.mean():.3f}')\n",
    "axes[0].set_xlabel('Prediction Confidence (Probability)', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title('Model Confidence Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Confidence by correctness\n",
    "correct_mask = y_test_np == predictions\n",
    "confidence_data = [max_probs[correct_mask], max_probs[~correct_mask]]\n",
    "tick_labels = ['Correct\\nPredictions', 'Incorrect\\nPredictions']\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "\n",
    "bp = axes[1].boxplot(confidence_data, tick_labels=tick_labels, patch_artist=True, \n",
    "                      showmeans=True, meanline=True)\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.6)\n",
    "axes[1].set_ylabel('Prediction Confidence', fontsize=12)\n",
    "axes[1].set_title('Confidence: Correct vs Incorrect Predictions', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"ðŸ“Š Confidence Statistics:\")\n",
    "print(f\"Mean confidence (all): {max_probs.mean():.3f}\")\n",
    "print(f\"Mean confidence (correct): {max_probs[correct_mask].mean():.3f}\")\n",
    "if (~correct_mask).sum() > 0:\n",
    "    print(f\"Mean confidence (incorrect): {max_probs[~correct_mask].mean():.3f}\")\n",
    "    print(f\"\\nðŸ’¡ Note: Lower confidence on incorrect predictions suggests model uncertainty!\")\n",
    "else:\n",
    "    print(f\"\\nðŸŽ‰ All predictions were correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ² Model Confidence Analysis\n",
    "\n",
    "Let's examine how confident the model is in its predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Detailed Performance Metrics\n",
    "\n",
    "Beyond simple accuracy, let's examine precision, recall, and F1-scores for each activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed performance metrics\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "\n",
    "print(\"ðŸ“Š Detailed Classification Report:\\n\")\n",
    "print(classification_report(y_test_np, enc.inverse_transform(model.predict(X_test).argmax(axis=1)), \n",
    "                          target_names=np.unique(y_test_np)))\n",
    "\n",
    "# Per-class analysis\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    y_test_np, \n",
    "    enc.inverse_transform(model.predict(X_test).argmax(axis=1)),\n",
    "    labels=np.unique(y_test_np)\n",
    ")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Per-Class Performance Analysis:\")\n",
    "for i, activity in enumerate(np.unique(y_test_np)):\n",
    "    print(f\"\\n{activity.upper()}:\")\n",
    "    print(f\"  Precision: {precision[i]:.2%} - Of all predicted '{activity}', {precision[i]:.0%} were correct\")\n",
    "    print(f\"  Recall:    {recall[i]:.2%} - Of all actual '{activity}', {recall[i]:.0%} were detected\")\n",
    "    print(f\"  F1-Score:  {f1[i]:.2%} - Harmonic mean of precision and recall\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Metric Definitions:\")\n",
    "print(\"  â€¢ Precision: When model predicts a class, how often is it right?\")\n",
    "print(\"  â€¢ Recall: Of all actual instances, how many did we find?\")\n",
    "print(\"  â€¢ F1-Score: Balance between precision and recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze misclassifications (predictions already computed above)\n",
    "misclassified = y_test_np != predictions\n",
    "\n",
    "print(f\"ðŸ” Misclassification Analysis:\")\n",
    "print(f\"Total test samples: {len(y_test_np)}\")\n",
    "print(f\"Misclassified: {misclassified.sum()}\")\n",
    "print(f\"Accuracy: {(~misclassified).sum() / len(y_test_np):.2%}\")\n",
    "\n",
    "if misclassified.sum() > 0:\n",
    "    print(f\"\\nâŒ Misclassification Breakdown:\")\n",
    "    misclass_df = pd.DataFrame({\n",
    "        'True': y_test_np[misclassified],\n",
    "        'Predicted': predictions[misclassified]\n",
    "    })\n",
    "    \n",
    "    print(misclass_df.groupby(['True', 'Predicted']).size().to_frame('Count').reset_index())\n",
    "    \n",
    "    # Most common confusion\n",
    "    if len(misclass_df) > 0:\n",
    "        most_common = misclass_df.groupby(['True', 'Predicted']).size().idxmax()\n",
    "        print(f\"\\nâš ï¸  Most common confusion: '{most_common[0]}' predicted as '{most_common[1]}'\")\n",
    "else:\n",
    "    print(\"\\nðŸŽ‰ Perfect classification - no mistakes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ” Analyzing Misclassifications\n",
    "\n",
    "Let's find out where the model makes mistakes and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ðŸš€ Real-World Testing: Sequential Activity Prediction\n",
    "\n",
    "Now let's test on a continuous recording that contains all activities in sequence. This simulates real-world usage!\n",
    "\n",
    "**Why this matters:**\n",
    "- In real applications, you don't get perfectly segmented activities\n",
    "- The model must handle transitions between activities\n",
    "- This tests if the model can maintain accuracy in continuous streaming scenarios\n",
    "\n",
    "**What we're doing:**\n",
    "- Using `X_test_all` - a continuous recording with all 4 activities performed in sequence\n",
    "- The model predicts activity every 256 timesteps (the window size)\n",
    "- We visualize predictions as colored bands overlaid on the actual sensor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_acc_gyro_RNN = pd.DataFrame(enc.inverse_transform(model.predict(X_test_all).argmax(axis=1)))\n",
    "plot_all_move_with_pred(X_test_all_stdev, predictions_acc_gyro_RNN, 'stdev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŽ¨ Understanding the Visualization\n",
    "\n",
    "**What to look for:**\n",
    "- **Smooth color regions** = confident, consistent predictions âœ…\n",
    "- **Rapid color changes** = model uncertainty or transition periods âš ï¸\n",
    "- **Color matching activity patterns** = correct predictions ðŸŽ¯\n",
    "- **Mismatched colors** = errors or transition confusion âŒ\n",
    "\n",
    "**Key Insights:**\n",
    "1. **Transition periods are challenging** - When switching between activities, the 256-timestep window contains mixed data\n",
    "2. **Model maintains context** - The LSTM's memory helps maintain predictions even through noisy data\n",
    "3. **Real-world readiness** - Consistent predictions across extended sequences indicate the model is production-ready\n",
    "4. **Windowing effects** - Each prediction uses the previous 256 samples, creating a slight lag in detecting activity changes\n",
    "\n",
    "**Performance Indicators:**\n",
    "- Long, stable color blocks = high confidence, accurate predictions\n",
    "- Clean transitions at activity boundaries = good temporal modeling\n",
    "- Any persistent misclassifications = areas for model improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š What We Learned\n",
    "\n",
    "### 1. **Data Understanding**\n",
    "- âœ… Always explore your data before modeling\n",
    "- âœ… Check class balance to avoid biased models\n",
    "- âœ… Visualize sensor patterns to understand what the model learns\n",
    "- âœ… Statistical analysis reveals important features\n",
    "\n",
    "### 2. **Model Architecture**\n",
    "- âœ… LSTMs are powerful for sequential/time-series data\n",
    "- âœ… Architecture choices (units, dropout) significantly impact performance\n",
    "- âœ… GRU is a simpler, faster alternative to LSTM\n",
    "- âœ… Model complexity should match problem complexity\n",
    "\n",
    "### 3. **Training & Evaluation**\n",
    "- âœ… Monitor both training and validation metrics\n",
    "- âœ… Overfitting = model memorizes training data (bad generalization)\n",
    "- âœ… Use confusion matrix to find patterns in mistakes\n",
    "- âœ… Confidence analysis reveals model uncertainty\n",
    "\n",
    "### 4. **Hyperparameter Tuning**\n",
    "- âœ… Timestep window size affects context vs samples tradeoff\n",
    "- âœ… Dropout prevents overfitting but too much causes underfitting\n",
    "- âœ… Experimentation is key to finding optimal settings\n",
    "\n",
    "### 5. **Real-World Application**\n",
    "- âœ… Testing on continuous sequences simulates real usage\n",
    "- âœ… Transition periods between activities can be challenging\n",
    "- âœ… Model confidence matters in production systems\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ Next Steps\n",
    "\n",
    "**Want to go further? Try these challenges:**\n",
    "\n",
    "1. **Add more activities** - Collect data for stairs, cycling, etc.\n",
    "2. **Real-time prediction** - Stream sensor data and predict live\n",
    "3. **Feature engineering** - Try FFT, wavelets, or other signal processing\n",
    "4. **Ensemble methods** - Combine multiple models for better accuracy\n",
    "5. **Transfer learning** - Pre-train on one person, fine-tune on another\n",
    "6. **Deployment** - Create a mobile app using TensorFlow Lite\n",
    "\n",
    "**Questions to explore:**\n",
    "- How does model performance vary across different people?\n",
    "- Can we detect activity transitions (e.g., walk â†’ run)?\n",
    "- What's the minimum sensor sampling rate needed?\n",
    "- Can we use only accelerometer (no gyroscope) and maintain accuracy?\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ Remember\n",
    "> \"Machine learning is iterative. Experiment, analyze, learn, and improve!\"\n",
    "\n",
    "Happy learning! ðŸŽ‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ðŸŽ“ Key Takeaways & Learning Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¬ Experiment 1: Compare LSTM vs GRU\n",
    "\n",
    "GRU (Gated Recurrent Unit) is a simpler alternative to LSTM. Let's compare them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build GRU model for comparison\n",
    "print(\"ðŸ”„ Training GRU model for comparison...\\n\")\n",
    "\n",
    "model_gru = Sequential()\n",
    "model_gru.add(Input(shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model_gru.add(GRU(units=n_timesteps))  # Using GRU instead of LSTM\n",
    "model_gru.add(Dropout(0.1))\n",
    "model_gru.add(Dense(units=4, activation='softmax'))\n",
    "model_gru.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train GRU model\n",
    "history_gru = model_gru.fit(X_train, y_train, epochs=10, shuffle=True, validation_split=0.1, verbose=0)\n",
    "\n",
    "# Evaluate both models\n",
    "ev_lstm = model.evaluate(X_test, y_test, verbose=0)\n",
    "ev_gru = model_gru.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Compare results\n",
    "print(\"ðŸ“Š Model Comparison:\")\n",
    "print(f\"\\nLSTM:\")\n",
    "print(f\"  Parameters: {model.count_params():,}\")\n",
    "print(f\"  Test Loss: {ev_lstm[0]:.4f}\")\n",
    "print(f\"  Test Accuracy: {ev_lstm[1]:.2%}\")\n",
    "\n",
    "print(f\"\\nGRU:\")\n",
    "print(f\"  Parameters: {model_gru.count_params():,}\")\n",
    "print(f\"  Test Loss: {ev_gru[0]:.4f}\")\n",
    "print(f\"  Test Accuracy: {ev_gru[1]:.2%}\")\n",
    "\n",
    "# Plot training comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "ax1.plot(history.history['accuracy'], label='LSTM Train', color='#3498db', linewidth=2)\n",
    "ax1.plot(history.history['val_accuracy'], label='LSTM Val', color='#3498db', linestyle='--', linewidth=2)\n",
    "ax1.plot(history_gru.history['accuracy'], label='GRU Train', color='#e74c3c', linewidth=2)\n",
    "ax1.plot(history_gru.history['val_accuracy'], label='GRU Val', color='#e74c3c', linestyle='--', linewidth=2)\n",
    "ax1.set_title('Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "ax2.plot(history.history['loss'], label='LSTM Train', color='#3498db', linewidth=2)\n",
    "ax2.plot(history.history['val_loss'], label='LSTM Val', color='#3498db', linestyle='--', linewidth=2)\n",
    "ax2.plot(history_gru.history['loss'], label='GRU Train', color='#e74c3c', linewidth=2)\n",
    "ax2.plot(history_gru.history['val_loss'], label='GRU Val', color='#e74c3c', linestyle='--', linewidth=2)\n",
    "ax2.set_title('Loss Comparison', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Insights:\")\n",
    "print(\"  â€¢ LSTM has more parameters (more complex)\")\n",
    "print(\"  â€¢ GRU is faster to train (fewer parameters)\")\n",
    "print(\"  â€¢ Performance is often similar for many tasks\")\n",
    "print(\"  â€¢ LSTM may be better for very long sequences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¬ Experiment 2: Effect of Timestep Window Size\n",
    "\n",
    "The timestep window determines how much history the model sees. Let's test different sizes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different timestep sizes\n",
    "print(\"ðŸ§ª Testing different timestep window sizes...\\n\")\n",
    "\n",
    "timestep_sizes = [64, 128, 256, 512]\n",
    "results = []\n",
    "\n",
    "for ts in timestep_sizes:\n",
    "    # Reshape data for this timestep size\n",
    "    X_train_ts = np.reshape(X_train_df[:X_train_df.shape[0]//ts*ts].to_numpy(), \n",
    "                           (X_train_df.shape[0] // ts, ts, X_train_df.shape[1]))\n",
    "    X_test_ts = np.reshape(X_test_df[:X_test_df.shape[0]//ts*ts].to_numpy(), \n",
    "                          (X_test_df.shape[0] // ts, ts, X_test_df.shape[1]))\n",
    "    \n",
    "    y_train_ts = keras.utils.to_categorical(\n",
    "        enc.fit_transform(y_train_df['class'][::ts][:X_train_ts.shape[0]].to_numpy())\n",
    "    )\n",
    "    y_test_ts = keras.utils.to_categorical(\n",
    "        enc.fit_transform(y_test_df['class'][::ts][:X_test_ts.shape[0]].to_numpy())\n",
    "    )\n",
    "    \n",
    "    # Build and train model\n",
    "    model_ts = Sequential()\n",
    "    model_ts.add(Input(shape=(X_train_ts.shape[1], X_train_ts.shape[2])))\n",
    "    model_ts.add(LSTM(units=ts))\n",
    "    model_ts.add(Dropout(0.1))\n",
    "    model_ts.add(Dense(units=4, activation='softmax'))\n",
    "    model_ts.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    history_ts = model_ts.fit(X_train_ts, y_train_ts, epochs=10, shuffle=True, \n",
    "                              validation_split=0.1, verbose=0)\n",
    "    \n",
    "    # Evaluate\n",
    "    ev_ts = model_ts.evaluate(X_test_ts, y_test_ts, verbose=0)\n",
    "    \n",
    "    results.append({\n",
    "        'Timesteps': ts,\n",
    "        'Accuracy': ev_ts[1],\n",
    "        'Loss': ev_ts[0],\n",
    "        'Parameters': model_ts.count_params(),\n",
    "        'Samples': X_train_ts.shape[0]\n",
    "    })\n",
    "    \n",
    "    print(f\"Timesteps: {ts:4d} | Accuracy: {ev_ts[1]:.2%} | Loss: {ev_ts[0]:.4f} | Params: {model_ts.count_params():,}\")\n",
    "\n",
    "# Visualize results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "ax1.plot(results_df['Timesteps'], results_df['Accuracy'], marker='o', linewidth=2, markersize=10, color='#2ecc71')\n",
    "ax1.set_xlabel('Timestep Window Size', fontsize=12)\n",
    "ax1.set_ylabel('Test Accuracy', fontsize=12)\n",
    "ax1.set_title('Accuracy vs Timestep Window Size', fontsize=14, fontweight='bold')\n",
    "ax1.grid(alpha=0.3)\n",
    "ax1.set_xscale('log', base=2)\n",
    "\n",
    "ax2.plot(results_df['Timesteps'], results_df['Parameters'], marker='s', linewidth=2, markersize=10, color='#e74c3c')\n",
    "ax2.set_xlabel('Timestep Window Size', fontsize=12)\n",
    "ax2.set_ylabel('Number of Parameters', fontsize=12)\n",
    "ax2.set_title('Model Complexity vs Timestep Size', fontsize=14, fontweight='bold')\n",
    "ax2.grid(alpha=0.3)\n",
    "ax2.set_xscale('log', base=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Insights:\")\n",
    "print(\"  â€¢ Larger windows = more context but fewer training samples\")\n",
    "print(\"  â€¢ Smaller windows = more samples but less temporal context\")\n",
    "print(\"  â€¢ Sweet spot depends on the activity's characteristic time scale\")\n",
    "print(f\"  â€¢ Best accuracy: {results_df.loc[results_df['Accuracy'].idxmax(), 'Timesteps']} timesteps \"\n",
    "      f\"({results_df['Accuracy'].max():.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¬ Experiment 3: Dropout Effect Analysis\n",
    "\n",
    "Dropout is a regularization technique. Let's see how it affects performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different dropout rates\n",
    "print(\"ðŸ§ª Testing different dropout rates...\\n\")\n",
    "\n",
    "dropout_rates = [0.0, 0.1, 0.2, 0.3, 0.5]\n",
    "dropout_results = []\n",
    "dropout_histories = []\n",
    "\n",
    "for dropout in dropout_rates:\n",
    "    model_dropout = Sequential()\n",
    "    model_dropout.add(Input(shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model_dropout.add(LSTM(units=n_timesteps))\n",
    "    if dropout > 0:\n",
    "        model_dropout.add(Dropout(dropout))\n",
    "    model_dropout.add(Dense(units=4, activation='softmax'))\n",
    "    model_dropout.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    history_dropout = model_dropout.fit(X_train, y_train, epochs=10, shuffle=True, \n",
    "                                       validation_split=0.1, verbose=0)\n",
    "    \n",
    "    ev_dropout = model_dropout.evaluate(X_test, y_test, verbose=0)\n",
    "    \n",
    "    dropout_results.append({\n",
    "        'Dropout': dropout,\n",
    "        'Train_Acc': history_dropout.history['accuracy'][-1],\n",
    "        'Val_Acc': history_dropout.history['val_accuracy'][-1],\n",
    "        'Test_Acc': ev_dropout[1],\n",
    "        'Overfitting': history_dropout.history['accuracy'][-1] - history_dropout.history['val_accuracy'][-1]\n",
    "    })\n",
    "    dropout_histories.append(history_dropout.history)\n",
    "    \n",
    "    print(f\"Dropout: {dropout:.1f} | Test Acc: {ev_dropout[1]:.2%} | \"\n",
    "          f\"Overfitting Gap: {dropout_results[-1]['Overfitting']:.2%}\")\n",
    "\n",
    "# Visualize\n",
    "dropout_df = pd.DataFrame(dropout_results)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "ax1.plot(dropout_df['Dropout'], dropout_df['Train_Acc'], marker='o', label='Train', linewidth=2, markersize=10)\n",
    "ax1.plot(dropout_df['Dropout'], dropout_df['Val_Acc'], marker='s', label='Validation', linewidth=2, markersize=10)\n",
    "ax1.plot(dropout_df['Dropout'], dropout_df['Test_Acc'], marker='^', label='Test', linewidth=2, markersize=10)\n",
    "ax1.set_xlabel('Dropout Rate', fontsize=12)\n",
    "ax1.set_ylabel('Accuracy', fontsize=12)\n",
    "ax1.set_title('Accuracy vs Dropout Rate', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "ax2.bar(dropout_df['Dropout'], dropout_df['Overfitting'], color='#e74c3c', alpha=0.7, edgecolor='black')\n",
    "ax2.set_xlabel('Dropout Rate', fontsize=12)\n",
    "ax2.set_ylabel('Overfitting Gap (Train - Val)', fontsize=12)\n",
    "ax2.set_title('Overfitting vs Dropout Rate', fontsize=14, fontweight='bold')\n",
    "ax2.axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Insights:\")\n",
    "print(\"  â€¢ Dropout = 0: No regularization (may overfit)\")\n",
    "print(\"  â€¢ Dropout too high: May underfit (loses important features)\")\n",
    "print(\"  â€¢ Sweet spot: Usually 0.1-0.3 for most tasks\")\n",
    "print(f\"  â€¢ Best test accuracy: {dropout_df.loc[dropout_df['Test_Acc'].idxmax(), 'Dropout']:.1f} \"\n",
    "      f\"({dropout_df['Test_Acc'].max():.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ðŸŒŸ Conclusion: From Fundamentals to the AI Revolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š What You've Learned\n",
    "\n",
    "Throughout this journey, you've built a complete understanding of machine learning fundamentals:\n",
    "\n",
    "### Core Techniques Mastered\n",
    "1. **Data Preparation** - Cleaning, splitting, and windowing time series data\n",
    "2. **Exploratory Data Analysis** - Visualizing patterns and understanding data distributions\n",
    "3. **Neural Network Architecture** - LSTM/GRU layers for sequential data processing\n",
    "4. **Model Training** - Optimization, loss functions, and the training loop\n",
    "5. **Evaluation Metrics** - Accuracy, precision, recall, F1-score, confusion matrices\n",
    "6. **Hyperparameter Tuning** - Testing timestep windows, dropout rates, architecture choices\n",
    "7. **Real-World Validation** - Sequential prediction on continuous data streams\n",
    "\n",
    "### The Scientific Method in Action\n",
    "You've experienced the iterative cycle of machine learning:\n",
    "```\n",
    "Hypothesis â†’ Experiment â†’ Analyze â†’ Learn â†’ Repeat\n",
    "```\n",
    "\n",
    "This is **exactly** how modern AI systems are developed - the same principles apply whether you're building a simple activity classifier or training GPT-4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¤– Connecting to Modern AI: From Your LSTM to ChatGPT\n",
    "\n",
    "### The Architecture Lineage\n",
    "\n",
    "**Your LSTM Model (2024):**\n",
    "- 256 timesteps of sensor data\n",
    "- LSTM layers with 256 hidden units\n",
    "- ~200K parameters\n",
    "- Classifies 4 activities with ~95% accuracy\n",
    "\n",
    "**Modern Large Language Models (2024-2025):**\n",
    "- Similar core architecture: **Transformers** (evolved from RNNs/LSTMs)\n",
    "- Thousands of tokens of context\n",
    "- Attention mechanisms (an advanced form of what LSTMs do)\n",
    "- Billions to trillions of parameters\n",
    "- Generate human-like text, code, images, and more\n",
    "\n",
    "### The Same Fundamental Principles\n",
    "\n",
    "The techniques you used in this lab are the **exact same foundations** powering today's AI revolution:\n",
    "\n",
    "| **Concept You Learned** | **How It's Used in Modern AI Tools** |\n",
    "|------------------------|-------------------------------------|\n",
    "| **Sequential processing (LSTM)** | Transformers process text sequences with self-attention |\n",
    "| **Timestep windows** | Context windows (GPT-4: 128K tokens, Claude: 200K tokens) |\n",
    "| **Dropout regularization** | Prevents overfitting in models with billions of parameters |\n",
    "| **Train/validation/test splits** | Essential for training reliable AI at any scale |\n",
    "| **Confusion matrices** | Used to evaluate model performance on specific tasks |\n",
    "| **Hyperparameter tuning** | Finding optimal learning rates, batch sizes, temperatures |\n",
    "| **Overfitting detection** | Critical when training models that can memorize training data |\n",
    "\n",
    "### Why Understanding These Basics Matters\n",
    "\n",
    "**You're not just using AI tools - you understand how they work:**\n",
    "\n",
    "1. **Critical Thinking** - When ChatGPT or GitHub Copilot makes a mistake, you understand *why* (hallucination, training data bias, overfitting to common patterns)\n",
    "\n",
    "2. **Better Prompting** - Understanding context windows and attention helps you craft better prompts (\"few-shot learning\" = giving examples in your prompt)\n",
    "\n",
    "3. **Model Selection** - You know when to use simpler models (like your LSTM) vs. when to leverage massive models (LLMs)\n",
    "\n",
    "4. **Ethical Awareness** - You've seen firsthand how models can misclassify, which extends to AI bias and fairness issues\n",
    "\n",
    "5. **Problem Decomposition** - You learned to break complex problems into data prep â†’ training â†’ evaluation â†’ iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ The Explosion of AI Tools (2023-2025)\n",
    "\n",
    "### The Current Landscape\n",
    "\n",
    "In just the past 2-3 years, we've witnessed an unprecedented acceleration:\n",
    "\n",
    "**2022:** ChatGPT launches (Nov 2022) - 100M users in 2 months\n",
    "\n",
    "**2023-2024:**\n",
    "- GPT-4, Claude 3, Gemini, LLaMA 2/3\n",
    "- GitHub Copilot, Cursor, Windsurf (AI pair programmers)\n",
    "- Midjourney, DALL-E 3, Stable Diffusion (image generation)\n",
    "- Video generation (Sora, Runway)\n",
    "- Voice cloning, music generation, 3D model creation\n",
    "\n",
    "**2025:** Multi-modal AI agents that can see, hear, reason, and act\n",
    "\n",
    "### Why Now? The Perfect Storm\n",
    "\n",
    "Your LSTM experiment reveals why AI exploded in the 2020s:\n",
    "\n",
    "1. **Data Availability** âœ…\n",
    "   - You had 28,672 training samples\n",
    "   - Modern LLMs trained on **trillions of tokens** from the internet\n",
    "\n",
    "2. **Computational Power** âœ…\n",
    "   - You trained on CPU/GPU in minutes\n",
    "   - LLMs trained on **thousands of GPUs** for months (cost: $100M+)\n",
    "\n",
    "3. **Algorithmic Innovations** âœ…\n",
    "   - You used LSTM (1997 invention)\n",
    "   - Transformers (2017) + scaling laws unlocked emergent abilities\n",
    "\n",
    "4. **Open Research** âœ…\n",
    "   - You built on published techniques\n",
    "   - Papers like \"Attention Is All You Need\" (2017) are freely available\n",
    "\n",
    "### The Scale Difference\n",
    "\n",
    "| **Your Project** | **GPT-4 (Estimated)** |\n",
    "|-----------------|----------------------|\n",
    "| 200K parameters | 1.8 **trillion** parameters |\n",
    "| 28K training samples | 13 trillion tokens (~10TB text) |\n",
    "| Minutes on laptop | Months on supercomputer |\n",
    "| $0 training cost | ~$100 million |\n",
    "| 6 sensor features | Understands text, code, images, more |\n",
    "\n",
    "**Yet the core math is the same!** Matrix multiplications, backpropagation, gradient descent, attention mechanisms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”® Looking Forward: The Future of AI\n",
    "\n",
    "### Emergent Trends (Late 2024 - 2025)\n",
    "\n",
    "1. **AI Agents** - Tools that can autonomously plan, reason, and act\n",
    "   - Example: This notebook may have been enhanced by an AI coding assistant\n",
    "   - Future: Agents that handle entire projects from specification to deployment\n",
    "\n",
    "2. **Multimodal AI** - Models that understand text, images, video, audio simultaneously\n",
    "   - Your sensor fusion (accelerometer + gyroscope) is a simple analogy!\n",
    "\n",
    "3. **Smaller, Specialized Models** - Not everything needs GPT-4\n",
    "   - Your 200K parameter LSTM is perfectly suited for its task\n",
    "   - Trend: \"Right-sized\" models for efficiency (edge devices, mobile)\n",
    "\n",
    "4. **AI-Assisted Learning** - The meta-moment\n",
    "   - You might be learning ML *with* AI tools helping you understand concepts\n",
    "   - AI becomes both the subject and the teacher\n",
    "\n",
    "### The Democratization of AI\n",
    "\n",
    "**What this means for you:**\n",
    "\n",
    "âœ… **Access** - You can use GPT-4, Claude, Gemini for free/cheap  \n",
    "âœ… **Education** - AI can explain concepts, debug code, suggest improvements  \n",
    "âœ… **Productivity** - GitHub Copilot writes boilerplate, catches bugs  \n",
    "âœ… **Creativity** - Generate images, music, ideas with AI assistance  \n",
    "âœ… **Building** - Fine-tune models, use APIs, deploy your own AI systems  \n",
    "\n",
    "**You're living through the \"electricity moment\" of AI** - a general-purpose technology that will transform every industry.\n",
    "\n",
    "### The Critical Skill: Understanding *When* and *How* to Use AI\n",
    "\n",
    "Just like you wouldn't use an LSTM for a simple linear regression, you need to know:\n",
    "- When to trust AI (well-defined, high-confidence tasks)\n",
    "- When to verify AI (critical decisions, safety-sensitive applications)\n",
    "- When to avoid AI (tasks requiring true creativity, ethical judgment)\n",
    "- How AI fails (hallucinations, bias, out-of-distribution data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’­ Final Reflection: You're Part of the Story\n",
    "\n",
    "### From Student to Practitioner\n",
    "\n",
    "By completing this notebook, you've:\n",
    "\n",
    "1. âœ… **Built a real ML system** from raw data to predictions\n",
    "2. âœ… **Understood the fundamentals** that power modern AI\n",
    "3. âœ… **Developed critical thinking** about model behavior and limitations\n",
    "4. âœ… **Gained hands-on experience** with the ML workflow\n",
    "5. âœ… **Connected theory to practice** in the context of today's AI revolution\n",
    "\n",
    "### The Human Element\n",
    "\n",
    "**AI tools are becoming exponentially more capable, but they still need humans who:**\n",
    "- Understand what problems are worth solving\n",
    "- Know how to evaluate model outputs critically\n",
    "- Can explain AI decisions to stakeholders\n",
    "- Consider ethical implications and societal impact\n",
    "- Combine domain expertise with technical knowledge\n",
    "\n",
    "**Your LSTM model achieved 95% accuracy on activity recognition. An LLM can write code, poetry, and jokes. But neither can:**\n",
    "- Decide what research directions are valuable\n",
    "- Feel empathy for users affected by automated decisions\n",
    "- Take responsibility when systems fail\n",
    "- Balance technical optimization with human values\n",
    "\n",
    "### The Meta-Moment ðŸ¤¯\n",
    "\n",
    "There's a fascinating irony here: **This conclusion section may have been written by an AI tool**, helping you understand AI itself. This is the world we're entering:\n",
    "- AI helps us learn about AI\n",
    "- AI helps us build better AI\n",
    "- Humans and AI collaborate on complex problems\n",
    "- The boundary between \"AI-generated\" and \"human-created\" blurs\n",
    "\n",
    "**But you still need to understand the fundamentals.** The model doesn't think for you - it amplifies your thinking.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Your Next Steps\n",
    "\n",
    "1. **Experiment** - Try different architectures, datasets, applications\n",
    "2. **Build** - Create ML projects that solve real problems\n",
    "3. **Share** - Contribute to open source, write about your learnings\n",
    "4. **Stay Curious** - The field evolves rapidly; keep learning\n",
    "5. **Think Critically** - Question AI outputs, understand limitations\n",
    "6. **Be Ethical** - Consider impacts on privacy, fairness, transparency\n",
    "\n",
    "### Resources for Continued Learning\n",
    "- **Papers:** [Attention Is All You Need](https://arxiv.org/abs/1706.03762) (Transformers)\n",
    "- **Courses:** fast.ai, Deeplearning.ai, Stanford CS231n\n",
    "- **Practice:** Kaggle competitions, HuggingFace models\n",
    "- **Community:** ML subreddits, Twitter/X #MachineLearning, Discord servers\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŒŸ Remember\n",
    "\n",
    "> **\"The best way to predict the future is to invent it.\"** - Alan Kay\n",
    "\n",
    "You've learned to build AI systems. You understand how they work, why they fail, and how to improve them. You're equipped to be part of the AI revolution - not just as a user, but as a creator, critic, and responsible practitioner.\n",
    "\n",
    "**The journey from LSTM to LLM is shorter than you think. You've taken the first steps.** ðŸš€\n",
    "\n",
    "---\n",
    "\n",
    "**Thank you for working through this lab. Now go build something amazing!** ðŸŽ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
