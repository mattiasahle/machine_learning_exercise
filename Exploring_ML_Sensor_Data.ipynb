{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d167a06",
   "metadata": {},
   "source": [
    "\n",
    "# Exploring Machine Learning through Sensor Data\n",
    "\n",
    "This notebook is a conceptual, hands-on exploration of machine learning using sensor data (accelerometer & gyroscope).  \n",
    "Follow the workflow: **load → explore → preprocess → feature analysis → model → reflect**.\n",
    "\n",
    "## Setting up your environment\n",
    "\n",
    "1. **Create and activate a Python virtual environment** (recommended):\n",
    "   - macOS / Linux:\n",
    "     ```bash\n",
    "     python -m venv ml_env\n",
    "     source ml_env/bin/activate\n",
    "     ```\n",
    "   - Windows (PowerShell):\n",
    "     ```powershell\n",
    "     python -m venv ml_env\n",
    "     .\\ml_env\\Scripts\\Activate.ps1\n",
    "     ```\n",
    "\n",
    "2. **Install dependencies** (from the folder containing `requirements.txt`):\n",
    "   ```bash\n",
    "   pip install -r requirements.txt\n",
    "   ```\n",
    "\n",
    "3. **Run the notebook**:\n",
    "   ```bash\n",
    "   jupyter notebook Exploring_ML_Sensor_Data.ipynb\n",
    "   ```\n",
    "   or:\n",
    "   ```bash\n",
    "   jupyter lab\n",
    "   ```\n",
    "\n",
    "4. **Using Visual Studio Code**:\n",
    "   - Install the **Python** and **Jupyter** extensions.\n",
    "   - Open the folder containing this notebook in VS Code, select the correct interpreter, open the `.ipynb`, then use the Run UI.\n",
    "\n",
    "5. **Google Colab (web option)**:\n",
    "   - Go to https://colab.research.google.com → Upload Notebook → choose this file.\n",
    "   - Optionally run `!pip install -r requirements.txt` at the top of the runtime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8feb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imports and configuration\n",
    "import os, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "sns.set(style='whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10,4)\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089402e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Environment check: print versions\n",
    "import sys, numpy as _np, pandas as _pd, sklearn as _sk, seaborn as _sns, matplotlib as _mpl\n",
    "try:\n",
    "    import tensorflow as _tf\n",
    "    tf_version = _tf.__version__\n",
    "except Exception:\n",
    "    tf_version = 'NOT INSTALLED'\n",
    "print('Python:', sys.version.splitlines()[0])\n",
    "print('NumPy:', _np.__version__)\n",
    "print('Pandas:', _pd.__version__)\n",
    "print('scikit-learn:', _sk.__version__)\n",
    "print('Seaborn:', _sns.__version__)\n",
    "print('Matplotlib:', _mpl.__version__)\n",
    "print('TensorFlow:', tf_version)\n",
    "print('\\nIf any are missing, run: pip install -r requirements.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cf0cee",
   "metadata": {},
   "source": [
    "\n",
    "## Overview\n",
    "\n",
    "This notebook guides you through a practical ML pipeline for sensor data:\n",
    "1. Load raw sensor data (CSV) or generate synthetic data\n",
    "2. Explore raw signals visually\n",
    "3. Preprocess: cleaning, smoothing, resampling, scaling\n",
    "4. Compare before/after preprocessing visually\n",
    "5. Explore and select features (manual selection encouraged)\n",
    "6. Train and compare simple models (KNN, Softmax/Logistic, SVM, Random Forest, optional LSTM)\n",
    "7. Reflect on results and robustness\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9546549c",
   "metadata": {},
   "source": [
    "## Synthetic data (optional)\n",
    "\n",
    "Run this cell to create a synthetic dataset that mimics accelerometer + gyroscope readings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2fa6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a synthetic dataset with X,Y,Z and GX,GY,GZ + labels and time\n",
    "classes = ['walk','run','jump','pushup']\n",
    "N = 500\n",
    "rows = []\n",
    "for cls in classes:\n",
    "    if cls == 'walk':\n",
    "        acc_base = [1.0, 1.2, 0.8]; gyr_base = [0.2, 0.1, 0.05]\n",
    "    elif cls == 'run':\n",
    "        acc_base = [3.0, 2.5, 3.5]; gyr_base = [0.6, 0.5, 0.4]\n",
    "    elif cls == 'jump':\n",
    "        acc_base = [5.0, 4.5, 6.0]; gyr_base = [0.8, 0.7, 0.6]\n",
    "    else:\n",
    "        acc_base = [0.5, 0.4, 0.6]; gyr_base = [0.15, 0.1, 0.05]\n",
    "    acc = np.random.normal(loc=acc_base, scale=0.5, size=(N,3))\n",
    "    gyr = np.random.normal(loc=gyr_base, scale=0.2, size=(N,3))\n",
    "    dfc = pd.DataFrame(np.hstack([acc,gyr]), columns=['X','Y','Z','GX','GY','GZ'])\n",
    "    dfc['label'] = cls\n",
    "    dfc['time'] = pd.date_range('2021-01-01', periods=N, freq='100ms')\n",
    "    dfc['source_file'] = f'synthetic_{cls}.csv'\n",
    "    rows.append(dfc)\n",
    "df_synthetic = pd.concat(rows, ignore_index=True)\n",
    "print('Synthetic dataset ready — shape:', df_synthetic.shape)\n",
    "df_synthetic.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13571b96",
   "metadata": {},
   "source": [
    "\n",
    "## Loading your CSV data (raw)\n",
    "\n",
    "Options:\n",
    "- **Auto-detection**: search a folder for CSVs (mode='auto')\n",
    "- **Manual**: provide a list of files (mode='manual')\n",
    "- **Synthetic**: use the included synthetic data (mode='synthetic')\n",
    "\n",
    "The loader **does not** preprocess — it simply reads raw CSVs so you can inspect and choose preprocessing steps yourself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbc9e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Flexible data loader\n",
    "import glob, os\n",
    "\n",
    "mode = 'synthetic'   # 'auto', 'manual', or 'synthetic'\n",
    "data_root = './data' # used for 'auto' mode\n",
    "manual_files = []    # e.g. ['./data/run_1.csv', './data/jump_1.csv']\n",
    "\n",
    "def load_sensor_csv(path):\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "    except Exception as e:\n",
    "        print('Failed to read', path, e)\n",
    "        return pd.DataFrame()\n",
    "    # Create unified time column if present\n",
    "    if 'Timestamp' in df.columns and 'Milliseconds' in df.columns:\n",
    "        df['time'] = pd.to_datetime(df['Timestamp']) + pd.to_timedelta(df['Milliseconds'], unit='ms')\n",
    "    elif 'Timestamp' in df.columns:\n",
    "        df['time'] = pd.to_datetime(df['Timestamp'])\n",
    "    else:\n",
    "        df['time'] = pd.RangeIndex(start=0, stop=len(df))\n",
    "    # Normalize column names (accept common variants)\n",
    "    colmap = {}\n",
    "    for c in df.columns:\n",
    "        cu = c.strip().upper()\n",
    "        if cu in ['X','AX']:\n",
    "            colmap[c] = 'X'\n",
    "        if cu in ['Y','AY']:\n",
    "            colmap[c] = 'Y'\n",
    "        if cu in ['Z','AZ']:\n",
    "            colmap[c] = 'Z'\n",
    "        if cu in ['GX']:\n",
    "            colmap[c] = 'GX'\n",
    "        if cu in ['GY']:\n",
    "            colmap[c] = 'GY'\n",
    "        if cu in ['GZ']:\n",
    "            colmap[c] = 'GZ'\n",
    "    df = df.rename(columns=colmap)\n",
    "    # infer label from filename (split on underscore)\n",
    "    basename = os.path.basename(path)\n",
    "    label = basename.split('_')[0]\n",
    "    df['label'] = label\n",
    "    df['source_file'] = basename\n",
    "    return df\n",
    "\n",
    "# Load according to mode\n",
    "if mode == 'synthetic':\n",
    "    df_raw = df_synthetic.copy()\n",
    "    print('Using synthetic data (mode=synthetic)')\n",
    "elif mode == 'auto':\n",
    "    files = glob.glob(os.path.join(data_root, '**', '*.csv'), recursive=True)\n",
    "    if not files:\n",
    "        print('No CSVs found under', data_root, '- falling back to synthetic')\n",
    "        df_raw = df_synthetic.copy()\n",
    "    else:\n",
    "        print('Found', len(files), 'CSV files. Loading...') \n",
    "        dfs = [load_sensor_csv(p) for p in files]\n",
    "        df_raw = pd.concat(dfs, ignore_index=True)\n",
    "        print('Combined rows:', len(df_raw))\n",
    "else:  # manual\n",
    "    if not manual_files:\n",
    "        print('No files in manual_files — using synthetic data instead')\n",
    "        df_raw = df_synthetic.copy()\n",
    "    else:\n",
    "        dfs = [load_sensor_csv(p) for p in manual_files]\n",
    "        df_raw = pd.concat(dfs, ignore_index=True)\n",
    "        print('Loaded manual files. Combined rows:', len(df_raw))\n",
    "\n",
    "print('\\nColumns available:', df_raw.columns.tolist())\n",
    "print('Sample labels:', pd.Series(df_raw.get('label', [])).unique()[:10])\n",
    "df_raw.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfe7ef1",
   "metadata": {},
   "source": [
    "\n",
    "### Reflection — Observing Raw Data\n",
    "\n",
    "Open the `df_raw.head()` output and descriptive stats. Look for:\n",
    "- Uneven timestamps or irregular sampling intervals\n",
    "- Missing values or NaNs\n",
    "- Inconsistent column names or scales between files\n",
    "\n",
    "Keep notes — these observations will inform your preprocessing choices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fccaf6",
   "metadata": {},
   "source": [
    "## Visual exploration of raw signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3895fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Quick visual exploration\n",
    "if 'df_raw' not in globals():\n",
    "    print('Run the data loader cell first.')\n",
    "else:\n",
    "    df = df_raw.copy()\n",
    "    if 'time' in df.columns:\n",
    "        df['time'] = pd.to_datetime(df['time'], errors='coerce')\n",
    "    print('Samples per label:'); display(df['label'].value_counts())\n",
    "    # Choose a label to visualize\n",
    "    lbl = df['label'].dropna().unique()[0] if 'label' in df.columns else None\n",
    "    if lbl is not None:\n",
    "        sample = df[df['label']==lbl].iloc[:1000]\n",
    "        plot_cols = [c for c in ['X','Y','Z','GX','GY','GZ'] if c in sample.columns]\n",
    "        if plot_cols:\n",
    "            plt.figure(figsize=(12,4))\n",
    "            for c in plot_cols:\n",
    "                sns.lineplot(data=sample, x='time' if 'time' in sample.columns else sample.index, y=c, label=c)\n",
    "            plt.title(f'Raw signals for label={lbl} (first samples)')\n",
    "            plt.show()\n",
    "    display(df.describe().T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568268d9",
   "metadata": {},
   "source": [
    "## Preprocessing: cleaning, smoothing, resampling, scaling\n",
    "\n",
    "This pipeline is intentionally *editable*. Try toggling options to see how each step affects the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a98188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocessing toggles (edit and re-run to experiment)\n",
    "fill_missing = True\n",
    "apply_smoothing = True\n",
    "apply_standardization = True\n",
    "resample_to_hz = None  # e.g., 20\n",
    "\n",
    "if 'df_raw' not in globals():\n",
    "    print('Run data loader first.')\n",
    "else:\n",
    "    df_processed = df_raw.copy()\n",
    "    if fill_missing:\n",
    "        df_processed = df_processed.fillna(method='ffill').fillna(method='bfill')\n",
    "    if resample_to_hz and 'time' in df_processed.columns:\n",
    "        out = []\n",
    "        for lbl, g in df_processed.groupby('label'):\n",
    "            g = g.set_index('time').sort_index()\n",
    "            g_res = g.resample(f'{int(1000/resample_to_hz)}L').mean().interpolate()\n",
    "            g_res['label'] = lbl\n",
    "            out.append(g_res.reset_index())\n",
    "        df_processed = pd.concat(out, ignore_index=True)\n",
    "        print(f'Resampled per label to {resample_to_hz} Hz')\n",
    "    if apply_smoothing:\n",
    "        cols = [c for c in ['X','Y','Z','GX','GY','GZ'] if c in df_processed.columns]\n",
    "        df_processed[cols] = df_processed[cols].rolling(window=5, min_periods=1).mean()\n",
    "    if apply_standardization:\n",
    "        num_cols = df_processed.select_dtypes(include=['number']).columns.tolist()\n",
    "        num_cols = [c for c in num_cols if c not in ['Milliseconds']]\n",
    "        scaler = StandardScaler()\n",
    "        df_processed[num_cols] = scaler.fit_transform(df_processed[num_cols])\n",
    "    print('Preprocessing complete — processed shape:', df_processed.shape)\n",
    "    display(df_processed.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0e521c",
   "metadata": {},
   "source": [
    "## Visual comparison: before vs after preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ba024e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compare raw vs processed visually (first 500 samples)\n",
    "if 'df_raw' not in globals() or 'df_processed' not in globals():\n",
    "    print('Run the loader and preprocessing cells first.')\n",
    "else:\n",
    "    cols = [c for c in ['X','Y','Z','GX','GY','GZ'] if c in df_raw.columns][:3]\n",
    "    if not cols:\n",
    "        cols = df_raw.select_dtypes('number').columns[:3].tolist()\n",
    "    fig, axes = plt.subplots(len(cols), 1, figsize=(12, 3*len(cols)), sharex=True)\n",
    "    for i, c in enumerate(cols):\n",
    "        ax = axes[i] if len(cols)>1 else axes\n",
    "        sns.lineplot(data=df_raw[c].iloc[:500], ax=ax, label='raw', linewidth=1)\n",
    "        sns.lineplot(data=df_processed[c].iloc[:500], ax=ax, label='processed', linewidth=1)\n",
    "        ax.set_ylabel(c); ax.legend()\n",
    "    plt.xlabel('sample index (first 500)')\n",
    "    plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b7a809",
   "metadata": {},
   "source": [
    "## Feature exploration: manual selection\n",
    "\n",
    "Edit the `selected_features` list to choose which features to analyze (pairplot + correlation heatmap)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3c02e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Edit this list to choose features to explore\n",
    "selected_features = ['X','Y','Z']  # change as needed based on your dataset\n",
    "avail = df_processed.columns.tolist() if 'df_processed' in globals() else []\n",
    "selected = [s for s in selected_features if s in avail]\n",
    "if not selected:\n",
    "    print('No valid selected features found in processed data. Update the list.')\n",
    "else:\n",
    "    print('Selected features:', selected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c76a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 'df_processed' in globals() and selected:\n",
    "    sns.pairplot(df_processed[selected].sample(min(1000, len(df_processed))), corner=True)\n",
    "    plt.suptitle('Pairplot of selected features', y=1.02)\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(df_processed[selected].corr(), annot=True, cmap='coolwarm', center=0)\n",
    "    plt.title('Correlation heatmap'); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225e6c8e",
   "metadata": {},
   "source": [
    "## From features to models\n",
    "\n",
    "Train simple models and observe differences conceptually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1233f9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare numeric features for modeling\n",
    "if 'df_processed' not in globals():\n",
    "    print('Run preprocessing first.')\n",
    "else:\n",
    "    num_cols = [c for c in df_processed.columns if c not in ['label','time','source_file','Milliseconds'] and pd.api.types.is_numeric_dtype(df_processed[c])]\n",
    "    if not num_cols:\n",
    "        raise ValueError('No numeric features found for modeling')\n",
    "    print('Numeric columns used for modeling:', num_cols[:12])\n",
    "    X = df_processed[num_cols].fillna(0).values\n",
    "    y = df_processed['label'].values\n",
    "    le = LabelEncoder(); y_enc = le.fit_transform(y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=SEED)\n",
    "    X_train_s = StandardScaler().fit_transform(X_train)\n",
    "    X_test_s = StandardScaler().fit_transform(X_test)\n",
    "    print('Prepared train/test datasets — shapes:', X_train_s.shape, X_test_s.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b052b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_s, y_train)\n",
    "knn_pred = knn.predict(X_test_s)\n",
    "print('KNN accuracy:', accuracy_score(y_test, knn_pred))\n",
    "print(classification_report(y_test, knn_pred))\n",
    "ConfusionMatrixDisplay(confusion_matrix(y_test, knn_pred), display_labels=le.classes_).plot(cmap='Blues')\n",
    "plt.title('KNN Confusion Matrix'); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6641c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Logistic Regression (Softmax)\n",
    "lr = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='saga')\n",
    "lr.fit(X_train_s, y_train)\n",
    "lr_pred = lr.predict(X_test_s)\n",
    "print('Logistic Regression accuracy:', accuracy_score(y_test, lr_pred))\n",
    "print(classification_report(y_test, lr_pred))\n",
    "ConfusionMatrixDisplay(confusion_matrix(y_test, lr_pred), display_labels=le.classes_).plot(cmap='Oranges')\n",
    "plt.title('Logistic Regression Confusion Matrix'); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42926e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SVM\n",
    "svm = SVC(kernel='rbf', C=1, gamma='scale')\n",
    "svm.fit(X_train_s, y_train)\n",
    "svm_pred = svm.predict(X_test_s)\n",
    "print('SVM accuracy:', accuracy_score(y_test, svm_pred))\n",
    "print(classification_report(y_test, svm_pred))\n",
    "ConfusionMatrixDisplay(confusion_matrix(y_test, svm_pred), display_labels=le.classes_).plot(cmap='Purples')\n",
    "plt.title('SVM Confusion Matrix'); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baa6d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=SEED)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict(X_test)\n",
    "print('Random Forest accuracy:', accuracy_score(y_test, rf_pred))\n",
    "print(classification_report(y_test, rf_pred))\n",
    "importances = pd.Series(rf.feature_importances_, index=num_cols).sort_values(ascending=False)\n",
    "plt.figure(figsize=(8,6)); importances[:12].plot(kind='barh'); plt.title('Top feature importances (Random Forest)'); plt.gca().invert_yaxis(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660defbd",
   "metadata": {},
   "source": [
    "## Optional: small LSTM demo (conceptual)\n",
    "\n",
    "This toy demo reshapes features into short sequences when possible; it's for conceptual understanding only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3511d84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LSTM demo (runs only when reshape is feasible)\n",
    "n_features = X_train_s.shape[1]\n",
    "if n_features >= 3 and n_features % 3 == 0:\n",
    "    timesteps = 3\n",
    "    step = n_features // timesteps\n",
    "    X_seq = X_train_s.reshape((X_train_s.shape[0], timesteps, step))\n",
    "    X_seq_test = X_test_s.reshape((X_test_s.shape[0], timesteps, step))\n",
    "    y_seq = le.transform(y_train)\n",
    "    y_seq_test = le.transform(y_test)\n",
    "    model = Sequential([LSTM(32, input_shape=(timesteps, step)), Dense(len(le.classes_), activation='softmax')])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X_seq, y_seq, epochs=4, batch_size=32, validation_split=0.1, verbose=2)\n",
    "    loss, acc = model.evaluate(X_seq_test, y_seq_test, verbose=0)\n",
    "    print('LSTM demo accuracy:', acc)\n",
    "else:\n",
    "    print('Skipping LSTM demo — reshape not suitable for current feature count')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b215ef",
   "metadata": {},
   "source": [
    "\n",
    "## Experiments & Reflection\n",
    "\n",
    "Try experiments:\n",
    "- Simulate pocket tightness by scaling accelerometer channels and re-run models.\n",
    "- Remove gyroscope features and compare performance.\n",
    "- Inject noise or drop segments and observe model robustness.\n",
    "- Use cross-validation across recording sessions to measure generalization.\n",
    "\n",
    "**Reflection prompts**:\n",
    "1. Which preprocessing step changed the signals most — and why?  \n",
    "2. Which model performed best — and can you explain why based on the features?  \n",
    "3. How would you adapt data collection or model design for a production system?\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
