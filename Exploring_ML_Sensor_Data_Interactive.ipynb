{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "973382d0",
   "metadata": {},
   "source": [
    "\n",
    "# Exploring Machine Learning through Sensor Data\n",
    "\n",
    "This notebook is a focused, practical walk-through of an ML workflow for sensor data.  \n",
    "It balances concise explanations with hands-on code and optional experiments so you can *learn by doing*.  \n",
    "\n",
    "**Notebook structure (brief):**  \n",
    "- Step 0 â€” Setting up the environment (quick, practical)  \n",
    "- Step 1 â€” Getting to know the data (load & visualize)  \n",
    "- Step 2 â€” Preparing the data (cleaning, smoothing, scaling)  \n",
    "- Step 3 â€” Feature exploration (manual selection & insight)  \n",
    "- Step 4 â€” Meet some learners (KNN, Logistic, SVM, Random Forest, optional LSTM)  \n",
    "- Step 5 â€” Reflection and next steps\n",
    "\n",
    "---\n",
    "\n",
    "## Step 0 â€” Setting up your environment (concise)\n",
    "\n",
    "1. Create and activate a virtual environment (recommended):\n",
    "   - macOS / Linux:\n",
    "     ```bash\n",
    "     python -m venv ml_env\n",
    "     source ml_env/bin/activate\n",
    "     ```\n",
    "   - Windows (PowerShell):\n",
    "     ```powershell\n",
    "     python -m venv ml_env\n",
    "     .\\ml_env\\Scripts\\Activate.ps1\n",
    "     ```\n",
    "\n",
    "2. Install required packages (run from folder containing `requirements.txt`):\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "3. Run the notebook:\n",
    "```bash\n",
    "jupyter notebook Exploring_ML_Sensor_Data.ipynb\n",
    "```\n",
    "or use the modern interface:\n",
    "```bash\n",
    "jupyter lab\n",
    "```\n",
    "\n",
    "**VS Code:** install the Python and Jupyter extensions, open the folder, pick the interpreter, then open this `.ipynb` and use the Run UI.\n",
    "\n",
    "**Google Colab:** Upload this notebook to Colab and (optionally) run `!pip install -r requirements.txt` at the top of the runtime.\n",
    "\n",
    "---\n",
    "\n",
    "Run the next code cell (imports) and then the Environment Check cell that follows it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe3e2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imports and basic configuration\n",
    "import os, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "sns.set(style='whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10,4)\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d5483c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Environment check: run this cell to verify core packages\n",
    "import sys, numpy as _np, pandas as _pd, sklearn as _sk, seaborn as _sns, matplotlib as _mpl\n",
    "try:\n",
    "    import tensorflow as _tf\n",
    "    tf_version = _tf.__version__\n",
    "except Exception:\n",
    "    tf_version = 'NOT INSTALLED'\n",
    "print('Python:', sys.version.splitlines()[0])\n",
    "print('NumPy:', _np.__version__)\n",
    "print('Pandas:', _pd.__version__)\n",
    "print('scikit-learn:', _sk.__version__)\n",
    "print('Seaborn:', _sns.__version__)\n",
    "print('Matplotlib:', _mpl.__version__)\n",
    "print('TensorFlow:', tf_version)\n",
    "print('\\nIf packages are missing, run: pip install -r requirements.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b8f695",
   "metadata": {},
   "source": [
    "\n",
    "## Step 1 â€” Getting to Know the Data\n",
    "\n",
    "First, we load data. You can either use the **synthetic dataset** included here (useful to experiment quickly) or **load your own CSV files**.\n",
    "\n",
    "The loader expects CSVs with typical sensor columns such as `Timestamp`, `Milliseconds`, `X`, `Y`, `Z` or variants. The loader will *not* preprocess your data â€” that comes later so you can explore raw behavior first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd292b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Synthetic dataset (optional) ---\n",
    "classes = ['walk','run','jump','pushup']\n",
    "N = 500\n",
    "frames = []\n",
    "for cls in classes:\n",
    "    if cls == 'walk':\n",
    "        acc_base = [1.0, 1.2, 0.8]; gyr_base = [0.2, 0.1, 0.05]\n",
    "    elif cls == 'run':\n",
    "        acc_base = [3.0, 2.5, 3.5]; gyr_base = [0.6, 0.5, 0.4]\n",
    "    elif cls == 'jump':\n",
    "        acc_base = [5.0, 4.5, 6.0]; gyr_base = [0.8, 0.7, 0.6]\n",
    "    else:\n",
    "        acc_base = [0.5, 0.4, 0.6]; gyr_base = [0.15, 0.1, 0.05]\n",
    "    acc = np.random.normal(loc=acc_base, scale=0.5, size=(N,3))\n",
    "    gyr = np.random.normal(loc=gyr_base, scale=0.2, size=(N,3))\n",
    "    dfc = pd.DataFrame(np.hstack([acc,gyr]), columns=['X','Y','Z','GX','GY','GZ'])\n",
    "    dfc['label'] = cls\n",
    "    dfc['time'] = pd.date_range('2021-01-01', periods=N, freq='100ms')\n",
    "    dfc['source_file'] = f'synthetic_{cls}.csv'\n",
    "    frames.append(dfc)\n",
    "df_synthetic = pd.concat(frames, ignore_index=True)\n",
    "print('Synthetic dataset ready â€” shape:', df_synthetic.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30082ff6",
   "metadata": {},
   "source": [
    "\n",
    "### Data loading options\n",
    "\n",
    "- **mode = 'synthetic'**: use the dataset created above.  \n",
    "- **mode = 'auto'**: recursively search `data_root` for CSV files and load them.  \n",
    "- **mode = 'manual'**: provide a `manual_files` list with explicit file paths.  \n",
    "\n",
    "*Try it yourself:* If you have a folder of CSVs, set `mode='auto'` and `data_root` to the parent folder. The loader will infer labels from filenames (splitting at `_`) and create a `time` column when `Timestamp` is available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b859bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Flexible loader (auto/manual/synthetic)\n",
    "import glob, os\n",
    "\n",
    "mode = 'synthetic'   # change to 'auto' or 'manual' as needed\n",
    "data_root = './data' # used for 'auto' mode\n",
    "manual_files = []    # list explicit paths when using manual mode, e.g. ['./data/run_1.csv']\n",
    "\n",
    "def load_sensor_csv(path):\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "    except Exception as e:\n",
    "        print('Failed to read', path, e); return pd.DataFrame()\n",
    "    # unified time column\n",
    "    if 'Timestamp' in df.columns and 'Milliseconds' in df.columns:\n",
    "        df['time'] = pd.to_datetime(df['Timestamp']) + pd.to_timedelta(df['Milliseconds'], unit='ms')\n",
    "    elif 'Timestamp' in df.columns:\n",
    "        df['time'] = pd.to_datetime(df['Timestamp'])\n",
    "    else:\n",
    "        df['time'] = pd.RangeIndex(start=0, stop=len(df))\n",
    "    # normalize column names\n",
    "    colmap = {}\n",
    "    for c in df.columns:\n",
    "        cu = c.strip().upper()\n",
    "        if cu in ['X','AX']: colmap[c] = 'X'\n",
    "        if cu in ['Y','AY']: colmap[c] = 'Y'\n",
    "        if cu in ['Z','AZ']: colmap[c] = 'Z'\n",
    "        if cu in ['GX']: colmap[c] = 'GX'\n",
    "        if cu in ['GY']: colmap[c] = 'GY'\n",
    "        if cu in ['GZ']: colmap[c] = 'GZ'\n",
    "    df = df.rename(columns=colmap)\n",
    "    basename = os.path.basename(path)\n",
    "    label = basename.split('_')[0]\n",
    "    df['label'] = label\n",
    "    df['source_file'] = basename\n",
    "    return df\n",
    "\n",
    "# load according to mode\n",
    "if mode == 'synthetic':\n",
    "    df_raw = df_synthetic.copy(); print('Using synthetic data (mode=synthetic)')\n",
    "elif mode == 'auto':\n",
    "    files = glob.glob(os.path.join(data_root, '**', '*.csv'), recursive=True)\n",
    "    if not files:\n",
    "        print('No CSVs found under', data_root, '- falling back to synthetic'); df_raw = df_synthetic.copy()\n",
    "    else:\n",
    "        print('Found', len(files), 'CSV files. Loading...'); dfs = [load_sensor_csv(p) for p in files]; df_raw = pd.concat(dfs, ignore_index=True)\n",
    "else:\n",
    "    if not manual_files:\n",
    "        print('manual_files empty â€” using synthetic data'); df_raw = df_synthetic.copy()\n",
    "    else:\n",
    "        dfs = [load_sensor_csv(p) for p in manual_files]; df_raw = pd.concat(dfs, ignore_index=True); print('Loaded manual files.')\n",
    "\n",
    "print('\\nColumns available:', df_raw.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff67e976",
   "metadata": {},
   "source": [
    "\n",
    "> **Note:** The data you just loaded is *raw*. It may contain missing values, inconsistent sampling intervals, or sensor drift.  \n",
    "> Donâ€™t preprocess yet â€” explore first. What stands out when you scan the first few rows?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690561d8",
   "metadata": {},
   "source": [
    "\n",
    "## Step 1b â€” Quick visual exploration\n",
    "\n",
    "Weâ€™ll plot a short excerpt to get a feel for the signals. Visual inspection often reveals problems that statistics alone can hide.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751dadea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Quick exploration: counts, summary, and a sample plot\n",
    "if 'df_raw' not in globals():\n",
    "    print('No data loaded yet â€” run the loader cell.')\n",
    "else:\n",
    "    df = df_raw.copy()\n",
    "    if 'time' in df.columns:\n",
    "        df['time'] = pd.to_datetime(df['time'], errors='coerce')\n",
    "    print('Samples per label:'); display(df['label'].value_counts())\n",
    "    # pick a label to visualize\n",
    "    lbl = df['label'].dropna().unique()[0] if 'label' in df.columns else None\n",
    "    if lbl is not None:\n",
    "        sample = df[df['label']==lbl].iloc[:1000]\n",
    "        plot_cols = [c for c in ['X','Y','Z','GX','GY','GZ'] if c in sample.columns]\n",
    "        if plot_cols:\n",
    "            plt.figure(figsize=(12,4))\n",
    "            for col in plot_cols:\n",
    "                sns.lineplot(data=sample, x='time' if 'time' in sample.columns else sample.index, y=col, label=col)\n",
    "            plt.title(f'Raw signals for label={lbl} (first samples)'); plt.show()\n",
    "    display(df.describe().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16129081",
   "metadata": {},
   "source": [
    "\n",
    "**ðŸ’¡ Try it yourself:** Want to explore different synthetic behavior? Increase noise or change `N` in the synthetic cell and re-run loader + exploration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35dfd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example (optional): increase noise for synthetic data\n",
    "# noise_scale = 1.2\n",
    "# acc = np.random.normal(loc=acc_base, scale=noise_scale, size=(N,3))\n",
    "# (recreate df_synthetic by re-running the synthetic data cell with adjusted noise)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05579e7",
   "metadata": {},
   "source": [
    "\n",
    "## Step 2 â€” Preparing the World for the Algorithm\n",
    "\n",
    "Preprocessing helps convert messy signals into a consistent form models can reason about. It is not about making data â€œperfectâ€ â€” it's about making important patterns clearer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc1c3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocessing toggles (editable)\n",
    "fill_missing = True\n",
    "apply_smoothing = True\n",
    "apply_standardization = True\n",
    "resample_to_hz = None  # e.g., 20\n",
    "\n",
    "if 'df_raw' not in globals():\n",
    "    print('Run data loader first.')\n",
    "else:\n",
    "    df_processed = df_raw.copy()\n",
    "    if fill_missing:\n",
    "        df_processed = df_processed.fillna(method='ffill').fillna(method='bfill')\n",
    "    if resample_to_hz and 'time' in df_processed.columns:\n",
    "        out = []\n",
    "        for lbl, g in df_processed.groupby('label'):\n",
    "            g = g.set_index('time').sort_index()\n",
    "            g_res = g.resample(f'{int(1000/resample_to_hz)}L').mean().interpolate()\n",
    "            g_res['label'] = lbl\n",
    "            out.append(g_res.reset_index())\n",
    "        df_processed = pd.concat(out, ignore_index=True)\n",
    "        print(f'Resampled to {resample_to_hz} Hz per label group')\n",
    "    if apply_smoothing:\n",
    "        cols = [c for c in ['X','Y','Z','GX','GY','GZ'] if c in df_processed.columns]\n",
    "        df_processed[cols] = df_processed[cols].rolling(window=5, min_periods=1).mean()\n",
    "    if apply_standardization:\n",
    "        num_cols = df_processed.select_dtypes(include=['number']).columns.tolist()\n",
    "        num_cols = [c for c in num_cols if c not in ['Milliseconds']]\n",
    "        scaler = StandardScaler()\n",
    "        df_processed[num_cols] = scaler.fit_transform(df_processed[num_cols])\n",
    "    print('Preprocessing done â€” processed shape:', df_processed.shape)\n",
    "    display(df_processed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd63f37e",
   "metadata": {},
   "source": [
    "\n",
    "**ðŸ’¡ Try it yourself:** Toggle `apply_smoothing=False` or set `resample_to_hz=20`. Observe how plots and summary statistics change.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7498bf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example: alternative smoothing (paste to test)\n",
    "# df_processed[['X','Y','Z']] = df_processed[['X','Y','Z']].rolling(window=15, min_periods=1).mean()\n",
    "# display(df_processed[['X','Y','Z']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0040a27",
   "metadata": {},
   "source": [
    "\n",
    "## Step 3 â€” Feature Exploration (manual selection)\n",
    "\n",
    "Choose features for deeper inspection. Look for redundancy, separability, or odd patterns that suggest sensor issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a505af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Feature selection (edit this list)\n",
    "selected_features = ['X','Y','Z']\n",
    "avail = df_processed.columns.tolist() if 'df_processed' in globals() else []\n",
    "selected = [s for s in selected_features if s in avail]\n",
    "if not selected:\n",
    "    print('No valid selected features found in processed data. Update the selected_features list.')\n",
    "else:\n",
    "    print('Selected features:', selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6064e501",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualization for selected features\n",
    "if 'df_processed' in globals() and selected:\n",
    "    sns.pairplot(df_processed[selected].sample(min(1000, len(df_processed))), corner=True)\n",
    "    plt.suptitle('Pairplot of selected features', y=1.02); plt.show()\n",
    "    plt.figure(figsize=(6,5)); sns.heatmap(df_processed[selected].corr(), annot=True, cmap='coolwarm', center=0); plt.title('Correlation heatmap'); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1afcb4",
   "metadata": {},
   "source": [
    "\n",
    "**ðŸ’¡ Try it yourself:** Add gyroscope axes (e.g., `'GX'`) to `selected_features`. Do those axes give new information or echo accelerometer axes?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093eff81",
   "metadata": {},
   "source": [
    "\n",
    "## Step 4 â€” Meet some learners\n",
    "\n",
    "Quick conceptual notes before running models:\n",
    "- **KNN**: uses nearby instances â€” good when similar movements cluster close in feature space.  \n",
    "- **Logistic (Softmax)**: simple linear decision boundaries, outputs class probabilities.  \n",
    "- **SVM**: finds strong boundaries, can be sensitive to scaling.  \n",
    "- **Random Forest**: ensembles of decision trees; useful for feature importance and robustness.  \n",
    "- **LSTM (optional)**: captures temporal patterns when data is framed as short sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec06065b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare numeric features and labels for modeling\n",
    "if 'df_processed' not in globals():\n",
    "    print('Run preprocessing first.')\n",
    "else:\n",
    "    num_cols = [c for c in df_processed.columns if c not in ['label','time','source_file','Milliseconds'] and pd.api.types.is_numeric_dtype(df_processed[c])]\n",
    "    if not num_cols:\n",
    "        raise ValueError('No numeric features found for modeling')\n",
    "    print('Numeric columns used for modeling:', num_cols[:12])\n",
    "    X = df_processed[num_cols].fillna(0).values\n",
    "    y = df_processed['label'].values\n",
    "    le = LabelEncoder(); y_enc = le.fit_transform(y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=SEED)\n",
    "    X_train_s = StandardScaler().fit_transform(X_train)\n",
    "    X_test_s = StandardScaler().fit_transform(X_test)\n",
    "    print('Prepared train/test datasets â€” shapes:', X_train_s.shape, X_test_s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84f8f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# K-Nearest Neighbors (KNN)\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_s, y_train)\n",
    "knn_pred = knn.predict(X_test_s)\n",
    "print('KNN accuracy:', accuracy_score(y_test, knn_pred))\n",
    "print(classification_report(y_test, knn_pred))\n",
    "ConfusionMatrixDisplay(confusion_matrix(y_test, knn_pred), display_labels=le.classes_).plot(cmap='Blues'); plt.title('KNN Confusion Matrix'); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a15442",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Logistic Regression (Softmax)\n",
    "lr = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='saga')\n",
    "lr.fit(X_train_s, y_train)\n",
    "lr_pred = lr.predict(X_test_s)\n",
    "print('Logistic Regression accuracy:', accuracy_score(y_test, lr_pred))\n",
    "print(classification_report(y_test, lr_pred))\n",
    "ConfusionMatrixDisplay(confusion_matrix(y_test, lr_pred), display_labels=le.classes_).plot(cmap='Oranges'); plt.title('Logistic Regression Confusion Matrix'); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023b4935",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Support Vector Machine (SVM)\n",
    "svm = SVC(kernel='rbf', C=1, gamma='scale')\n",
    "svm.fit(X_train_s, y_train)\n",
    "svm_pred = svm.predict(X_test_s)\n",
    "print('SVM accuracy:', accuracy_score(y_test, svm_pred))\n",
    "print(classification_report(y_test, svm_pred))\n",
    "ConfusionMatrixDisplay(confusion_matrix(y_test, svm_pred), display_labels=le.classes_).plot(cmap='Purples'); plt.title('SVM Confusion Matrix'); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab35716",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=SEED)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict(X_test)\n",
    "print('Random Forest accuracy:', accuracy_score(y_test, rf_pred))\n",
    "print(classification_report(y_test, rf_pred))\n",
    "importances = pd.Series(rf.feature_importances_, index=num_cols).sort_values(ascending=False)\n",
    "plt.figure(figsize=(8,6)); importances[:12].plot(kind='barh'); plt.title('Top feature importances (Random Forest)'); plt.gca().invert_yaxis(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff75c0f",
   "metadata": {},
   "source": [
    "## Optional â€” small LSTM demo (conceptual)\n",
    "\n",
    "This reshapes features into very short sequences if feasible. It's a toy demo to illustrate temporal models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84746d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LSTM demo (runs only when reshape is feasible)\n",
    "n_features = X_train_s.shape[1]\n",
    "if n_features >= 3 and n_features % 3 == 0:\n",
    "    timesteps = 3; step = n_features // timesteps\n",
    "    X_seq = X_train_s.reshape((X_train_s.shape[0], timesteps, step))\n",
    "    X_seq_test = X_test_s.reshape((X_test_s.shape[0], timesteps, step))\n",
    "    y_seq = le.transform(y_train); y_seq_test = le.transform(y_test)\n",
    "    model = Sequential([LSTM(32, input_shape=(timesteps, step)), Dense(len(le.classes_), activation='softmax')])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X_seq, y_seq, epochs=4, batch_size=32, validation_split=0.1, verbose=2)\n",
    "    loss, acc = model.evaluate(X_seq_test, y_seq_test, verbose=0)\n",
    "    print('LSTM demo accuracy:', acc)\n",
    "else:\n",
    "    print('Skipping LSTM demo â€” reshape not suitable for current feature count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9740251b",
   "metadata": {},
   "source": [
    "\n",
    "## Step 5 â€” Reflection and next steps\n",
    "\n",
    "**Questions to consider:**  \n",
    "- Which preprocessing step changed the signals most â€” and why?  \n",
    "- Which model seemed most robust to noise, scaling, or pocket variations?  \n",
    "- If you were to deploy a model on-device, which preprocessing pipeline and model would you choose?\n",
    "\n",
    "**Ideas for further exploration:**  \n",
    "- Implement session-wise cross-validation to test generalization.  \n",
    "- Try handcrafted features (e.g., standard deviation over windows) and compare with raw inputs.  \n",
    "- Explore more advanced sequence models or on-device optimizations.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
